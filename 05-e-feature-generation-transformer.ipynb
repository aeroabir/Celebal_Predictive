{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_models_ts import (create_padding_mask,\n",
    "                                   create_look_ahead_mask,\n",
    "                                   Transformer)\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    # (batch_size, 1, 1, seq_length, num_features)\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    # (batch_size, 1, 1, seq_length, num_features)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input\n",
    "    # received by the decoder.\n",
    "    # (seq_len, seq_len)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "\n",
    "    # dec_target_padding_mask = create_padding_mask(tar)\n",
    "    # combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "def evaluate_one_example(encoder_input, decoder_input, transformer):\n",
    "\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)  # (1, seq_len, features)\n",
    "    output = tf.expand_dims(decoder_input, 0)  # (1, features)\n",
    "    output = tf.expand_dims(output, 0)  # (1, 1, features)\n",
    "#     print('inside evaluate_one_example:')\n",
    "#     print(encoder_input.shape, decoder_input.shape, output.shape)\n",
    "    MAX_LENGTH = encoder_input.shape[1]\n",
    "    output = tf.cast(output, tf.float32)\n",
    "    \n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        # print(i, encoder_input.shape, output.shape)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     None,\n",
    "                                                     None,\n",
    "                                                     None)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, features), float32\n",
    "#         print(predictions.dtype, predictions.shape)\n",
    "#         print(output.dtype, output.shape)\n",
    "        output = tf.concat([output, predictions], axis=-2)  # concat on sequence\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def get_ae_features(encoder_input, transformer, embed_dim, batch_size):\n",
    "\n",
    "    num_points, seq_len, num_features = encoder_input.shape\n",
    "    # output = tf.zeros(shape=(num_points, seq_len, embed_dim))\n",
    "    output_list = []\n",
    "    if num_points > 100:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((encoder_input))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        for batch, inp in tqdm(enumerate(dataset)):\n",
    "            out = transformer.encoder(inp, training=False, mask=None)\n",
    "            # i0 = batch_size * batch\n",
    "            # i1 = i0 + batch_size\n",
    "            output_list.append(out)\n",
    "        output = tf.concat(output_list, axis=0)\n",
    "    else:\n",
    "        output = transformer.encoder(encoder_input, training=False, mask=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, checkpoint_path, **kwargs):\n",
    "\n",
    "    # typical example, num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    # input_vocab_size=8500, target_vocab_size=8000,\n",
    "    # pe_input=10000, pe_target=6000\n",
    "    num_layers = kwargs.get(\"num_layers\", 2)\n",
    "    d_model = kwargs.get(\"d_model\", 128)\n",
    "    num_heads = kwargs.get(\"num_heads\", 8)\n",
    "    dff = kwargs.get(\"dff\", 128)\n",
    "    BATCH_SIZE = kwargs.get(\"BATCH_SIZE\", 64)\n",
    "    epochs = kwargs.get(\"epochs\", 100)\n",
    "    action = kwargs.get(\"action\", \"train\")\n",
    "    padding_reqd = kwargs.get(\"padding_reqd\", False)\n",
    "    normalize_flag = kwargs.get(\"normalize_flag\", True)\n",
    "    seed = kwargs.get(\"seed\", 100)\n",
    "\n",
    "    xpoints, inp_seq_len, inp_features = X_train.shape\n",
    "    ypoints, out_seq_len, out_features = Y_train.shape\n",
    "    pe_input = inp_seq_len\n",
    "    pe_target = out_seq_len\n",
    "    target_vocab_size = out_features\n",
    "\n",
    "    print(\"Train and Test data shape:\", X_train.shape, X_test.shape)\n",
    "    BUFFER_SIZE = len(X_train)\n",
    "    steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    # val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "    # sample for testing\n",
    "    sample_transformer = Transformer(num_layers=num_layers,\n",
    "                                     d_model=d_model,\n",
    "                                     num_heads=num_heads, dff=dff,\n",
    "                                     target_vocab_size=out_features,\n",
    "                                     pe_input=inp_seq_len,\n",
    "                                     pe_target=out_seq_len)\n",
    "\n",
    "    temp_input = tf.random.uniform((BATCH_SIZE, inp_seq_len, inp_features),\n",
    "                                   dtype=tf.float32, minval=0, maxval=1)\n",
    "    temp_target = tf.random.uniform((BATCH_SIZE, out_seq_len, out_features),\n",
    "                                    dtype=tf.float32, minval=0, maxval=1)\n",
    "\n",
    "    fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                                   enc_padding_mask=None,\n",
    "                                   look_ahead_mask=None,\n",
    "                                   dec_padding_mask=None)\n",
    "    print(\"Output shape:\", fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    print(temp_input.dtype, temp_target.dtype)\n",
    "\n",
    "    # d_model % self.num_heads == 0\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              target_vocab_size, pe_input, pe_target, rate=0.1)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,\n",
    "                                         beta_2=0.999, epsilon=1e-7)\n",
    "    loss_object = tf.keras.losses.MeanSquaredError(\n",
    "                    reduction=tf.keras.losses.Reduction.NONE,\n",
    "                    name='mean_squared_error')\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        loss_ = loss_object(real, pred)  # batch X seq_len\n",
    "        loss_ = tf.reduce_sum(loss_, axis=-1)  # batch\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    def accuracy_function(real, pred):\n",
    "        mae = tf.keras.losses.MeanAbsoluteError(\n",
    "                reduction=tf.keras.losses.Reduction.NONE)\n",
    "        loss_ = mae(real, pred)\n",
    "        loss_ = tf.reduce_sum(loss_, axis=-1)\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                              checkpoint_path,\n",
    "                                              max_to_keep=5)\n",
    "    checkpoint_directory = os.path.dirname(checkpoint_path)\n",
    "    # if a checkpoint exists, restore the latest checkpoint.\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        print('Latest checkpoint restored!!')\n",
    "        \n",
    "    return_dict = {}\n",
    "\n",
    "    if action == \"train\":\n",
    "        # The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "        # execution. The function specializes to the precise shape of the argument\n",
    "        # tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "        # batch sizes (the last batch is smaller), use input_signature to specify\n",
    "        # more generic shapes.\n",
    "\n",
    "        train_step_signature = [\n",
    "            tf.TensorSpec(shape=(None, None, inp_features), dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=(None, None, out_features), dtype=tf.float64),\n",
    "        ]\n",
    "\n",
    "        @tf.function(input_signature=train_step_signature)\n",
    "        def train_step(inp, tar):\n",
    "            tar_inp = tar[:, :-1, :]\n",
    "            tar_real = tar[:, 1:, :]\n",
    "\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "            enc_padding_mask = None\n",
    "            dec_padding_mask = None\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions, _ = transformer(inp, tar_inp,\n",
    "                                            True,\n",
    "                                            enc_padding_mask,\n",
    "                                            combined_mask,\n",
    "                                            dec_padding_mask)\n",
    "                loss = loss_function(tar_real, predictions)\n",
    "\n",
    "            gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "            train_loss(loss)\n",
    "            train_accuracy(accuracy_function(tar_real, predictions))\n",
    "\n",
    "        train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "        print(f\"Training on {BUFFER_SIZE} data points over {epochs} epochs\")\n",
    "        t = trange(epochs, desc='Epoch Desc', leave=True)\n",
    "        \n",
    "        loss_dict = {'mae': [], 'mse': []}\n",
    "        for epoch in t:\n",
    "            start = time.time()\n",
    "\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "            # inp -> portuguese, tar -> english\n",
    "            for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "\n",
    "                # check for dimensions\n",
    "                # print(inp.shape, tar.shape)\n",
    "                # print(inp.dtype, tar.dtype)\n",
    "                # tar_inp = tar[:, :-1, :]\n",
    "                # enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "                # predictions, _ = transformer(inp, tar_inp, True, None, combined_mask, None)\n",
    "                # print(predictions.shape)\n",
    "\n",
    "                train_step(inp, tar)\n",
    "\n",
    "                # if batch % 50 == 0:\n",
    "                #     print('Epoch {} Batch {} MSE-Loss {:.4f} MAE-Loss {:.4f}'.format(\n",
    "                #         epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "            mse_loss, mae_loss = train_loss.result(), train_accuracy.result()\n",
    "            loss_dict['mse'].append(mse_loss)\n",
    "            loss_dict['mae'].append(mae_loss)\n",
    "            t.set_description('Epoch {} MSE-Loss {:.6f}  MAE-Loss {:.6f}'.format(epoch + 1,\n",
    "                                                                mse_loss,\n",
    "                                                                mae_loss))\n",
    "            t.refresh()\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                ckpt_save_path = ckpt_manager.save()\n",
    "                # print('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "            # print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "            #                                             train_loss.result(), \n",
    "            #                                             train_accuracy.result()))\n",
    "\n",
    "            # print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "        print(\"Training Complete!\")\n",
    "        return_dict['history'] = loss_dict\n",
    "\n",
    "    elif action == \"predict_all\":\n",
    "\n",
    "        pred = get_ae_features(X_train, transformer, d_model, BATCH_SIZE)\n",
    "        return_dict['prediction'] = pred\n",
    "        \n",
    "    elif action == \"predict_batch\":\n",
    "        # ckpt.restore(checkpoint_path).expect_partial()\n",
    "        dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(BATCH_SIZE, drop_remainder=False)\n",
    "        example_X, example_Y = next(iter(dataset_test))\n",
    "        print(example_X.shape, example_Y.shape)\n",
    "        predictions = np.zeros((BATCH_SIZE, out_seq_len, out_features), dtype=np.float64)\n",
    "        \n",
    "        for index in trange(BATCH_SIZE):\n",
    "            enc_inp, dec_inp0 = example_X[index, :, :], example_Y[index, 0, :]\n",
    "            pred_ii, _ = evaluate_one_example(enc_inp, dec_inp0, transformer)\n",
    "            predictions[index, :, :] = pred_ii\n",
    "            # predictions = np.append(predictions, pred_seq, axis=0)\n",
    "\n",
    "        return_dict['prediction'] = predictions\n",
    "        nsample = 5\n",
    "        len_x, len_y = example_X.shape[1], example_Y.shape[1]\n",
    "        shifted_x = [x+len_x for x in range(len_y)]\n",
    "        # indices = random.sample([ii for ii in range(example_X.shape[0])], k=nsample)\n",
    "        for ii in range(nsample):\n",
    "            filename = \"image_\"+str(ii)+\".png\"\n",
    "            plt.figure(ii+1)\n",
    "            cnum = random.randint(0, len(example_X)-1)\n",
    "            colnum = random.randint(0, out_features-1)\n",
    "            print(ii, cnum, colnum)\n",
    "            # input\n",
    "            plt.plot(example_X[cnum, :, colnum])\n",
    "\n",
    "            # target\n",
    "            plt.plot(shifted_x, example_Y[cnum, :, colnum])  # , 'bo--'\n",
    "\n",
    "            # prediction\n",
    "            plt.plot(shifted_x, predictions[cnum, :, colnum])  # , 'r+'\n",
    "            plt.savefig(os.path.join(checkpoint_directory, filename))\n",
    "            plt.title(\"Feature-\"+str(colnum))\n",
    "            print(f\"saved {filename}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Unknown action type:\", action)\n",
    "    \n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def plot_sample(example_X, example_Y, predictions, num_samples=1):\n",
    "    len_x, len_y = example_X.shape[1], example_Y.shape[1]\n",
    "    shifted_x = [x+len_x for x in range(len_y)]\n",
    "    for ii in range(num_samples):\n",
    "        cnum = random.randint(0, len(example_X)-1)\n",
    "        f = plt.figure(ii+1, figsize=(15,4))\n",
    "        ax1 = f.add_subplot(131)\n",
    "        ax2 = f.add_subplot(132)\n",
    "        ax3 = f.add_subplot(133)\n",
    "\n",
    "        ax1.plot(example_X[cnum,:, 0])\n",
    "        ax1.plot(shifted_x, example_Y[cnum,:,0])  # , 'bo--'\n",
    "        ax1.plot(shifted_x, predictions[cnum,:,0])  # , 'r+'\n",
    "        ax1.title.set_text('Power')\n",
    "\n",
    "        ax2.plot(example_X[cnum,:, 1])\n",
    "        ax2.plot(shifted_x, example_Y[cnum,:,1])  # , 'bo--'\n",
    "        ax2.plot(shifted_x, predictions[cnum,:,1])  # , 'r+'\n",
    "        ax2.title.set_text('Temperature')\n",
    "\n",
    "        ax3.plot(example_X[cnum,:, 2])\n",
    "        ax3.plot(shifted_x, example_Y[cnum,:,2])  # , 'bo--'\n",
    "        ax3.plot(shifted_x, predictions[cnum,:,2])  # , 'r+'\n",
    "        ax3.title.set_text('Irradiance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 04-c file\n",
    "# 'autoencoder-data-v01.pkl' - for past 1 day data with only operational hours\n",
    "# normalizing parameters\n",
    "# [ 539.6          70.15598297 6227.027832  ]\n",
    "# [ 539.6          70.78488922 6227.027832  ]\n",
    "\n",
    "# 'autoencoder-data-1d.pkl' - without removing non-operational hours\n",
    "# normalizing parameters\n",
    "# [ 539.6          70.15598297 6227.027832  ]\n",
    "# [ 539.6          70.78488922 18674.02929688]\n",
    "with open('autoencoder-data-1d.pkl', 'rb') as handle:\n",
    "    x_all, y_all = pickle.load(handle)\n",
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.amax(x_all, axis=(0, 1))\n",
    "ymax = np.amax(y_all, axis=(0, 1))\n",
    "print(xmax)\n",
    "print(ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = x_all / xmax\n",
    "y_norm = y_all / ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test, Y_train, Y_test = train_test_split(x_norm, y_norm, test_size=0.2)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "ckpt_path = os.path.join(data_dir, \"transformer_model_1d/cp.ckpt\")\n",
    "\n",
    "train(X_train,\n",
    "      Y_train,\n",
    "      X_test,\n",
    "      Y_test,\n",
    "      ckpt_path,\n",
    "      num_layers=2,\n",
    "      d_model=128,\n",
    "      epochs=500,\n",
    "      BATCH_SIZE=32,\n",
    "      action=\"train\",\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train(X_train, Y_train, X_test, Y_test, ckpt_path,\n",
    "                      num_layers=2,\n",
    "                      d_model=128,\n",
    "                      epochs=500,\n",
    "                      BATCH_SIZE=32,\n",
    "                      action=\"predict_batch\",\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(32, drop_remainder=False)\n",
    "example_X, example_Y = next(iter(dataset_test))\n",
    "print(example_X.shape, example_Y.shape, predictions['prediction'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of One Day Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(example_X, example_Y, predictions['prediction'], num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction for Operational Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(example_X, example_Y, predictions['prediction'], num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Transformer for Feature/EMbedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the alarm classification data\n",
    "with open('inverter-data-cnn-daily.pkl', 'rb') as handle:\n",
    "    x_dict, y_dict, label_df = pickle.load(handle)\n",
    "    \n",
    "x_all, y_all = [], []\n",
    "for inv in x_dict:\n",
    "    x_ii, y_ii = x_dict[inv], y_dict[inv]\n",
    "    x_all.append(x_ii)\n",
    "    y_all.append(y_ii)\n",
    "\n",
    "x_all = np.concatenate(x_all, axis=0)\n",
    "y_all = np.concatenate(y_all, axis=0)\n",
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "# ckpt_path = os.path.join(data_dir, \"transformer_model/cp.ckpt\")\n",
    "ckpt_path = os.path.join(data_dir, \"transformer_model_1d/cp.ckpt\")\n",
    "\n",
    "predictions = train(x_all, x_all, x_all, x_all, ckpt_path,\n",
    "                      num_layers=2,\n",
    "                      d_model=128,\n",
    "                      epochs=500,\n",
    "                      BATCH_SIZE=32,\n",
    "                      action=\"predict_all\",\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embedding is for every time-step\n",
    "    - dimension of the embedding = [:, sequence_length, embedding_dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed = predictions['prediction']\n",
    "x_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - one can choose either the first one (similar to BERT) or the last one for embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed_final = x_embed[:, -1, :]\n",
    "x_embed_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed_initial = x_embed[:, 0, :]\n",
    "x_embed_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "x_tsne = TSNE(n_components=2).fit_transform(x_embed_final)\n",
    "x_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.DataFrame()\n",
    "df_subset['tsne-2d-one'] = x_tsne[:,0]\n",
    "df_subset['tsne-2d-two'] = x_tsne[:,1]\n",
    "df_subset[\"y\"] = y_all\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 2),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE plot reveals that the embeddings are not really discriminating between the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (x_embed_final, y_all, label_df)\n",
    "with open('inverter-data-daily-embedded-final.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tsne2 = TSNE(n_components=2).fit_transform(x_embed_initial)\n",
    "x_tsne2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pd.DataFrame()\n",
    "df_subset['tsne-2d-one'] = x_tsne2[:,0]\n",
    "df_subset['tsne-2d-two'] = x_tsne2[:,1]\n",
    "df_subset[\"y\"] = y_all\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=sns.color_palette(\"hls\", 2),\n",
    "    data=df_subset,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (x_embed_initial, y_all, label_df)\n",
    "with open('inverter-data-daily-embedded-initial.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_sample(x, y, num_samples=5):\n",
    "    pos_indices = np.where(y==1)\n",
    "    neg_indices = np.where(y==0)\n",
    "    \n",
    "    x_pos = x[pos_indices[0], :, :]\n",
    "    x_neg = x[neg_indices[0], :, :]\n",
    "    \n",
    "    print(x_pos.shape)\n",
    "    print(len(pos_indices[0]), len(neg_indices[0]))\n",
    "    \n",
    "    for ii in range(num_samples):\n",
    "        pindx = random.randint(0, len(pos_indices[0])-1)\n",
    "        nindx = random.randint(0, len(neg_indices[0])-1)\n",
    "        \n",
    "        pnum = pos_indices[0][pindx]\n",
    "        nnum = neg_indices[0][nindx]\n",
    "        print(y[pnum], y[nnum])\n",
    "        \n",
    "        f = plt.figure(ii+1, figsize=(15,4))\n",
    "        ax1 = f.add_subplot(131)\n",
    "        ax2 = f.add_subplot(132)\n",
    "        ax3 = f.add_subplot(133)\n",
    "\n",
    "        ax1.plot(x[pnum, :, 0], label='pos')\n",
    "        ax1.plot(x[nnum, :, 0], label='neg')\n",
    "        ax1.title.set_text('Power')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "\n",
    "        ax2.plot(x[pnum, :, 1], label='pos')\n",
    "        ax2.plot(x[nnum, :, 1], label='neg')\n",
    "        ax2.title.set_text('Temperature')\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "\n",
    "        ax3.plot(x[pnum, :, 2], label='pos')\n",
    "        ax3.plot(x[nnum, :, 2], label='neg')\n",
    "        ax3.title.set_text('Irradiance')\n",
    "        ax3.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_sample(x_all, y_all, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
