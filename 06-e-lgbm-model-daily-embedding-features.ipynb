{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building for Hourly/Daily Predictions\n",
    "\n",
    "    - labels are created separately (see 04-label-generation.ipynb)\n",
    "    - labels are for alarms generated in the next 5 minutes, next 1 hour and next 1 day\n",
    "    - data points can be at 5 minutes interval, hourly level and day lavel\n",
    "    \n",
    "    - this notebook uses embeddings generated from Transformer models\n",
    "    - Transformer model returns [:, sequence_length, embedding_dimension] shaped embeddings\n",
    "    - for the current model - the last one or the first one in the sequence is used, i.e., [:, -1, :] or [:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-v03.pkl', 'rb') as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "    \n",
    "print(all_data.shape)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-daily-embedded-initial.pkl', 'rb') as handle:\n",
    "    embed_data = pickle.load(handle)\n",
    "x_emb, y, label_df = embed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'label_24h'\n",
    "total = label_df[label_col].value_counts()\n",
    "print(f\"Class ratio: {100 * total[1] / (total[0] + total[1]):.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame()\n",
    "all_data[label_col] = y\n",
    "all_data.reset_index(level=0, inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(all_data, train_size=0.8, random_state=100)\n",
    "train, valid = train_test_split(train, train_size=0.8, random_state=100)\n",
    "print(train.shape, valid.shape, test.shape)\n",
    "\n",
    "train[label_col].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb = np.array(x_emb)\n",
    "\n",
    "row_idx = np.array(train.index, dtype=np.intp)\n",
    "col_idx = np.arange(x_emb.shape[1])\n",
    "# col_idx = np.array([0, 2], dtype=np.intp)\n",
    "\n",
    "x_train = x_emb[row_idx[:, np.newaxis], col_idx]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = np.array(valid.index, dtype=np.intp)\n",
    "col_idx = np.arange(x_emb.shape[1])\n",
    "x_valid = x_emb[row_idx[:, np.newaxis], col_idx]\n",
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, x_train, y_train, x_valid, y_valid, categoricals, fixed_params, param_set={}, verbose_eval=50):\n",
    "        self.categoricals = categoricals\n",
    "        self.fixed_params = fixed_params\n",
    "        self.param_set = param_set\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.dtrain = lgb.Dataset(\n",
    "            x_train,\n",
    "            label = y_train,\n",
    "            categorical_feature=self.categoricals,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.dvalid = lgb.Dataset(\n",
    "            x_valid,\n",
    "            label = y_valid,\n",
    "            categorical_feature=self.categoricals,\n",
    "            reference=self.dtrain,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.default_ranges = {\n",
    "            \"num_leaves\":(2, 256),\n",
    "            \"min_data_in_leaf\":(5, 100),\n",
    "            \"learning_rate\":(1e-3, 1e-1),\n",
    "            \"feature_fraction\":(0.4, 1.0),\n",
    "            \"bagging_freq\":(1, 7),\n",
    "            \"bagging_fraction\":(0.4, 1.0)\n",
    "        }\n",
    "        \n",
    "    def get_params(self, trial):\n",
    "        param_funcs = {\n",
    "            \"num_leaves\":trial.suggest_int,\n",
    "            \"min_data_in_leaf\":trial.suggest_int,\n",
    "            \"learning_rate\":trial.suggest_loguniform,\n",
    "            \"feature_fraction\":trial.suggest_float,\n",
    "            \"bagging_freq\":trial.suggest_int,\n",
    "            \"bagging_fraction\":trial.suggest_float\n",
    "        }\n",
    "        params = {}\n",
    "        for param, rng in self.param_set.items():\n",
    "            if rng is None:\n",
    "                default_rng = self.default_ranges[param]\n",
    "                params[param] = param_funcs[param](param, default_rng[0], default_rng[1])\n",
    "            else:\n",
    "                params[param] = param_funcs[param](param, rng[0], rng[1])\n",
    "\n",
    "        params.update(self.fixed_params)\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        params = self.get_params(trial)\n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            self.dtrain,\n",
    "            valid_sets=[self.dvalid],\n",
    "            verbose_eval=self.verbose_eval\n",
    "        )\n",
    "        # get best value of objective\n",
    "        valid_0 = bst.best_score['valid_0']\n",
    "        score = valid_0[list(valid_0)[0]]\n",
    "        \n",
    "        trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "        trial.set_user_attr('features', self.dtrain.feature_name)\n",
    "        trial.set_user_attr('importance', bst.feature_importance().tolist())\n",
    "        \n",
    "        return score\n",
    "\n",
    "class EarlyStoppingExceeded(optuna.exceptions.OptunaError):\n",
    "    pass\n",
    "\n",
    "class EarlyStoppingCallback(object):\n",
    "    # from https://github.com/optuna/optuna/issues/1001#issuecomment-596478792\n",
    "    \n",
    "    def __init__(self, early_stopping_rounds, min_delta):\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.min_delta = min_delta\n",
    "        self.early_stopping_count = 0\n",
    "        self.best_score = None\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        if self.best_score == None:\n",
    "            self.best_score = study.best_value\n",
    "\n",
    "        if study.best_value < self.best_score - self.min_delta:\n",
    "            self.best_score = study.best_value\n",
    "            self.early_stopping_count = 0\n",
    "        else:\n",
    "            if self.early_stopping_count > self.early_stopping_rounds:\n",
    "                self.early_stopping_count = 0\n",
    "                best_score = None\n",
    "                raise EarlyStoppingExceeded()\n",
    "            else:\n",
    "                self.early_stopping_count += 1\n",
    "        return\n",
    "    \n",
    "\n",
    "def tune_model(x_train, y_train, x_valid, y_valid, categoricals, fixed_params, param_set, n_trials=50, verbose_eval=50, show_progress=True, early_stop_callback=None, tpe_mode=\"independent\"):\n",
    "    multivariate_flag = True if tpe_mode == \"multivariate\" else False\n",
    "    sampler = optuna.samplers.TPESampler(multivariate=multivariate_flag)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    callbacks = None\n",
    "    if early_stop_callback is not None:\n",
    "        callbacks = [early_stop_callback]\n",
    "    else:\n",
    "        callbacks = []\n",
    "    try:\n",
    "        study.optimize(\n",
    "            Objective(\n",
    "                x_train=x_train,\n",
    "                y_train=y_train,\n",
    "                x_valid=x_valid,\n",
    "                y_valid=y_valid,\n",
    "                categoricals=categoricals,\n",
    "                fixed_params=fixed_params,\n",
    "                param_set = param_set,\n",
    "                verbose_eval=verbose_eval\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=show_progress,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except EarlyStoppingExceeded:\n",
    "        print(f'EarlyStopping Exceeded: No new best scores on iters {early_stop_callback.early_stopping_rounds}')\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = 'binary'\n",
    "num_rounds = 1000\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "print(\"Tune hyperparameters...\")\n",
    "param_set = {\n",
    "    \"num_leaves\":None, \n",
    "    \"min_data_in_leaf\":None, \n",
    "    \"learning_rate\":None, \n",
    "    \"feature_fraction\":None,\n",
    "    \"bagging_freq\":None, \n",
    "    \"bagging_fraction\":None\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    \"objective\":obj_func,\n",
    "    \"metric\":[obj_func, \"auc\"],\n",
    "    \"num_rounds\":num_rounds,\n",
    "    \"early_stopping_rounds\":early_stopping_rounds,\n",
    "    \"first_metric_only\":True,\n",
    "    \"force_row_wise\":True,\n",
    "    \"feature_pre_filter\":False,\n",
    "    \"verbose\":1,\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(10, 0.001)\n",
    "categoricals = []\n",
    "study = tune_model(\n",
    "                    x_train, train[label_col],\n",
    "                    x_valid, valid[label_col],\n",
    "                    categoricals, fixed_params, param_set, n_trials=100, verbose_eval=0,\n",
    "                    show_progress=False, early_stop_callback=early_stopping,\n",
    "                )\n",
    "\n",
    "print(\"Saving best model parameters...\")\n",
    "best_params = {k: [v] for (k,v) in study.best_params.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters:', best_params)\n",
    "num_rounds = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "\n",
    "fixed_params[\"num_rounds\"] = num_rounds\n",
    "# fixed_params[\"early_stopping_rounds\"] = 0\n",
    "params = study.best_params.copy()\n",
    "\n",
    "params.update(fixed_params)\n",
    "del params[\"early_stopping_rounds\"] # = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['verbose'] = 1\n",
    "params['metric'] = ['binary', 'auc']\n",
    "# params['is_unbalance'] = True\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           num_leaves=params['num_leaves'], \n",
    "                           min_data_in_leaf=params['min_data_in_leaf'],\n",
    "                           learning_rate=params['learning_rate'],\n",
    "                           feature_fraction=params['feature_fraction'],\n",
    "                           bagging_freq=params['bagging_freq'],\n",
    "                           bagging_fraction=params['bagging_fraction'],\n",
    "                           objective='binary',\n",
    "                           metric=params['metric'],\n",
    "                           num_rounds=params['num_rounds'],\n",
    "#                            is_unbalance=params['is_unbalance']\n",
    "                          )\n",
    "y_train = train[label_col]\n",
    "y_val = valid[label_col]\n",
    "\n",
    "model.fit(X=x_train, y=y_train, \n",
    "          eval_set=[(x_valid, y_val)],\n",
    "          eval_names=['eval']\n",
    "         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = np.array(test.index, dtype=np.intp)\n",
    "col_idx = np.arange(x_emb.shape[1])\n",
    "x_test = x_emb[row_idx[:, np.newaxis], col_idx]\n",
    "print(x_test.shape)\n",
    "y_test = test[label_col]\n",
    "\n",
    "train_pred = model.predict_proba(x_train)\n",
    "val_pred = model.predict_proba(x_valid)\n",
    "test_pred = model.predict_proba(x_test)\n",
    "train_pred.shape, val_pred.shape, test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_true=y_train, y_score=train_pred[:,1])\n",
    "val_auc = roc_auc_score(y_true=y_val, y_score=val_pred[:,1])\n",
    "test_auc = roc_auc_score(y_true=y_test, y_score=test_pred[:,1])\n",
    "print(f\"train-auc: {train_auc}, val-auc: {val_auc}, test-auc: {test_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve for the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(y_test, test_pred[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_pred[:,1])\n",
    "# disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "# disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "#                    'AP={0:0.2f}'.format(average_precision))\n",
    "line_kwargs1 = {\"drawstyle\": \"steps-post\", 'label': 'precision'}\n",
    "line_kwargs2 = {\"drawstyle\": \"steps-post\", 'label': 'threshold'}\n",
    "# line_kwargs = {}\n",
    "# line_kwargs[\"label\"] = ('precision', 'threshold')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs1)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs2)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel, title=f\"Average Precision = {average_precision:0.2f}\")\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
