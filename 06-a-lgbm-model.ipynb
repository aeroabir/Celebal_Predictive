{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-v02.pkl', 'rb') as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "    \n",
    "print(all_data.shape)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['hour', \"day\", \"dayofweek\", \"weekofyear\", \"month\"]\n",
    "for cat in categoricals:\n",
    "    all_data[cat] = all_data[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = all_data['label'].value_counts()\n",
    "print(f\"Class ratio: {100 * total[1] / (total[0] + total[1]):.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.80\n",
    "# train_count = math.floor(all_data.shape[0] * train_ratio)\n",
    "# train_data = all_data.sample(n=train_count, random_state=100)\n",
    "# train_days, eval_days = 60, 60\n",
    "train_days, eval_days = 120, 90\n",
    "dmin, dmax = all_data.date.min(), all_data.date.max()\n",
    "train_end = dmax - pd.Timedelta(eval_days, 'D')\n",
    "train_start = train_end - pd.Timedelta(train_days, 'D')\n",
    "print(dmin, dmax, train_start, train_end)\n",
    "\n",
    "# random split\n",
    "train, test = train_test_split(all_data, train_size=0.8, random_state=100)\n",
    "\n",
    "# train, test = all_data[all_data.date < train_end], all_data[all_data.date >= train_end]\n",
    "# train, test = all_data[(all_data.date >= train_start) & (all_data.date < train_end)], all_data[all_data.date >= train_end]\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "train['label'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificially increase the class ratio\n",
    "class_ratio = 0.01\n",
    "total = train['label'].value_counts()\n",
    "print(f\"Train class ratio: {100 * total[1] / (total[0] + total[1]):.3f} %\")\n",
    "\n",
    "num_positive = total[1]\n",
    "num_negative = math.floor((num_positive / class_ratio) * (1 - class_ratio))\n",
    "print(num_positive, num_negative)\n",
    "\n",
    "# create a new dataset\n",
    "train_pos_rows = train[train['label'] == 1]\n",
    "train_neg_rows = train[train['label'] == 0]\n",
    "sampled = train_neg_rows.sample(n=num_negative, random_state=100)\n",
    "print(sampled.shape)\n",
    "\n",
    "# new train data\n",
    "train_data = pd.concat([train_pos_rows, sampled], axis=0)\n",
    "total = train_data['label'].value_counts()\n",
    "print(f\"Modified class ratio: {100 * total[1] / (total[0] + total[1]):.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(train_data, train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, df_train, df_valid, categoricals, fixed_params, param_set={}, verbose_eval=50):\n",
    "        self.categoricals = categoricals\n",
    "        self.fixed_params = fixed_params\n",
    "        self.param_set = param_set\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.dtrain = lgb.Dataset(\n",
    "            df_train.drop([\"label\"], axis=1),\n",
    "            label = df_train[\"label\"],\n",
    "            categorical_feature=self.categoricals,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.dvalid = lgb.Dataset(\n",
    "            df_valid.drop([\"label\"], axis=1),\n",
    "            label = df_valid[\"label\"],\n",
    "            categorical_feature=self.categoricals,\n",
    "            reference=self.dtrain,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.default_ranges = {\n",
    "            \"num_leaves\":(2, 256),\n",
    "            \"min_data_in_leaf\":(5, 100),\n",
    "            \"learning_rate\":(1e-3, 1e-1),\n",
    "            \"feature_fraction\":(0.4, 1.0),\n",
    "            \"bagging_freq\":(1, 7),\n",
    "            \"bagging_fraction\":(0.4, 1.0)\n",
    "        }\n",
    "        \n",
    "    def get_params(self, trial):\n",
    "        param_funcs = {\n",
    "            \"num_leaves\":trial.suggest_int,\n",
    "            \"min_data_in_leaf\":trial.suggest_int,\n",
    "            \"learning_rate\":trial.suggest_loguniform,\n",
    "            \"feature_fraction\":trial.suggest_float,\n",
    "            \"bagging_freq\":trial.suggest_int,\n",
    "            \"bagging_fraction\":trial.suggest_float\n",
    "        }\n",
    "        params = {}\n",
    "        for param, rng in self.param_set.items():\n",
    "            if rng is None:\n",
    "                default_rng = self.default_ranges[param]\n",
    "                params[param] = param_funcs[param](param, default_rng[0], default_rng[1])\n",
    "            else:\n",
    "                params[param] = param_funcs[param](param, rng[0], rng[1])\n",
    "\n",
    "        params.update(self.fixed_params)\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        params = self.get_params(trial)\n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            self.dtrain,\n",
    "            valid_sets=[self.dvalid],\n",
    "            verbose_eval=self.verbose_eval\n",
    "        )\n",
    "        # get best value of objective\n",
    "        valid_0 = bst.best_score['valid_0']\n",
    "        score = valid_0[list(valid_0)[0]]\n",
    "        \n",
    "        trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "        trial.set_user_attr('features', self.dtrain.feature_name)\n",
    "        trial.set_user_attr('importance', bst.feature_importance().tolist())\n",
    "        \n",
    "        return score\n",
    "\n",
    "class EarlyStoppingExceeded(optuna.exceptions.OptunaError):\n",
    "    pass\n",
    "\n",
    "class EarlyStoppingCallback(object):\n",
    "    # from https://github.com/optuna/optuna/issues/1001#issuecomment-596478792\n",
    "    \n",
    "    def __init__(self, early_stopping_rounds, min_delta):\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.min_delta = min_delta\n",
    "        self.early_stopping_count = 0\n",
    "        self.best_score = None\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        if self.best_score == None:\n",
    "            self.best_score = study.best_value\n",
    "\n",
    "        if study.best_value < self.best_score - self.min_delta:\n",
    "            self.best_score = study.best_value\n",
    "            self.early_stopping_count = 0\n",
    "        else:\n",
    "            if self.early_stopping_count > self.early_stopping_rounds:\n",
    "                self.early_stopping_count = 0\n",
    "                best_score = None\n",
    "                raise EarlyStoppingExceeded()\n",
    "            else:\n",
    "                self.early_stopping_count += 1\n",
    "        return\n",
    "    \n",
    "\n",
    "def tune_model(df_train, df_valid, categoricals, fixed_params, param_set, n_trials=50, verbose_eval=50, show_progress=True, early_stop_callback=None, tpe_mode=\"independent\"):\n",
    "    multivariate_flag = True if tpe_mode == \"multivariate\" else False\n",
    "    sampler = optuna.samplers.TPESampler(multivariate=multivariate_flag)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    callbacks = None\n",
    "    if early_stop_callback is not None:\n",
    "        callbacks = [early_stop_callback]\n",
    "    else:\n",
    "        callbacks = []\n",
    "    try:\n",
    "        study.optimize(\n",
    "            Objective(\n",
    "                df_train=df_train,\n",
    "                df_valid=df_valid,\n",
    "                categoricals=categoricals,\n",
    "                fixed_params=fixed_params,\n",
    "                param_set = param_set,\n",
    "                verbose_eval=verbose_eval\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=show_progress,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except EarlyStoppingExceeded:\n",
    "        print(f'EarlyStopping Exceeded: No new best scores on iters {early_stop_callback.early_stopping_rounds}')\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = 'binary'\n",
    "num_rounds = 1000\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "print(\"Tune hyperparameters...\")\n",
    "param_set = {\n",
    "    \"num_leaves\":None, \n",
    "    \"min_data_in_leaf\":None, \n",
    "    \"learning_rate\":None, \n",
    "    \"feature_fraction\":None,\n",
    "    \"bagging_freq\":None, \n",
    "    \"bagging_fraction\":None\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    \"objective\":obj_func,\n",
    "    \"metric\":[obj_func, \"auc\"],\n",
    "    \"num_rounds\":num_rounds,\n",
    "    \"early_stopping_rounds\":early_stopping_rounds,\n",
    "    \"first_metric_only\":True,\n",
    "    \"force_row_wise\":True,\n",
    "    \"feature_pre_filter\":False,\n",
    "    \"verbose\":1,\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(10, 0.001)\n",
    "\n",
    "study = tune_model(\n",
    "                    df_train.drop(columns=[\"date\", \"inverter\"]),\n",
    "                    df_valid.drop(columns=[\"date\", \"inverter\"]),\n",
    "                    categoricals, fixed_params, param_set, n_trials=100, verbose_eval=0,\n",
    "                    show_progress=False, early_stop_callback=early_stopping,\n",
    "                )\n",
    "\n",
    "print(\"Saving best model parameters...\")\n",
    "best_params = {k: [v] for (k,v) in study.best_params.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_params.pkl', 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters:', best_params)\n",
    "num_rounds = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "\n",
    "fixed_params[\"num_rounds\"] = num_rounds\n",
    "# fixed_params[\"early_stopping_rounds\"] = 0\n",
    "params = study.best_params.copy()\n",
    "\n",
    "params.update(fixed_params)\n",
    "del params[\"early_stopping_rounds\"] # = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['verbose'] = 1\n",
    "params['metric'] = ['binary', 'auc']\n",
    "# params['is_unbalance'] = True\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           num_leaves=params['num_leaves'], \n",
    "                           min_data_in_leaf=params['min_data_in_leaf'],\n",
    "                           learning_rate=params['learning_rate'],\n",
    "                           feature_fraction=params['feature_fraction'],\n",
    "                           bagging_freq=params['bagging_freq'],\n",
    "                           bagging_fraction=params['bagging_fraction'],\n",
    "                           objective='binary',\n",
    "                           metric=params['metric'],\n",
    "                           num_rounds=params['num_rounds'],\n",
    "#                            is_unbalance=params['is_unbalance']\n",
    "                          )\n",
    "x_train, y_train = df_train.drop(columns=[\"label\", \"date\", \"inverter\"]), df_train[\"label\"]\n",
    "x_val, y_val = df_valid.drop(columns=[\"label\", \"date\", \"inverter\"]), df_valid[\"label\"]\n",
    "\n",
    "model.fit(X=x_train, y=y_train, \n",
    "          eval_set=[(x_val, y_val)],\n",
    "          eval_names=['eval']\n",
    "         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test.drop(columns=[\"label\", \"date\", \"inverter\"]), test[\"label\"]\n",
    "\n",
    "train_pred = model.predict_proba(x_train)\n",
    "val_pred = model.predict_proba(x_val)\n",
    "test_pred = model.predict_proba(x_test)\n",
    "train_pred.shape, val_pred.shape, test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_, model.importance_type, model.classes_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_true=y_train, y_score=train_pred[:,1])\n",
    "val_auc = roc_auc_score(y_true=y_val, y_score=val_pred[:,1])\n",
    "test_auc = roc_auc_score(y_true=y_test, y_score=test_pred[:,1])\n",
    "print(f\"train-auc: {train_auc}, val-auc: {val_auc}, test-auc: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "feature_imp = pd.DataFrame({'Value':model.feature_importances_,\n",
    "                            'Feature':df_train.drop(columns=[\"label\", \"date\", \"inverter\"]).columns})\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.set(font_scale = 2)\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                    ascending=False)[0:num])\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances-01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(y_test, test_pred[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_pred[:,1])\n",
    "# disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "# disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "#                    'AP={0:0.2f}'.format(average_precision))\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "# line_kwargs = {}\n",
    "line_kwargs[\"label\"] = (f\"AP = {average_precision:0.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.where(test_pred[:,1]>0.4, 1, 0)\n",
    "print(classification_report(y_test, yhat, target_names=['Normal', 'Faulty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(y_val, val_pred[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_pred[:,1])\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "# line_kwargs = {}\n",
    "line_kwargs[\"label\"] = (f\"AP = {average_precision:0.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = lgb.Dataset(\n",
    "#                 df_train.drop(columns=[\"date\", \"label\"]),\n",
    "#                 label=df_train[\"label\"],\n",
    "#                 categorical_feature=categoricals\n",
    "#             )\n",
    "# bst = lgb.train(params, dtrain, verbose_eval=1)\n",
    "\n",
    "# train_pred = bst.predict(df_train.drop(columns=[\"label\", \"date\"]))\n",
    "# test_pred = bst.predict(df_test.drop(columns=[\"label\", \"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
