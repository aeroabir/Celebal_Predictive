{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-cnn-v01.pkl', 'rb') as handle:\n",
    "    x_dict, y_dict, label_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = [], []\n",
    "for inv in x_dict:\n",
    "    x_ii, y_ii = x_dict[inv], y_dict[inv]\n",
    "    x_all.append(x_ii)\n",
    "    y_all.append(y_ii)\n",
    "\n",
    "x_all = np.concatenate(x_all, axis=0)\n",
    "y_all = np.concatenate(y_all, axis=0)\n",
    "x_all.shape, y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_all = np.expand_dims(y_all, axis=-1)\n",
    "# y_all = tf.one_hot(y_all, depth=2)\n",
    "# y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=100)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.one_hot(y_train, depth=2)\n",
    "y_val = tf.one_hot(y_val, depth=2)\n",
    "y_test = tf.one_hot(y_test, depth=2)\n",
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_RESNET:\n",
    "\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, batch_size,\n",
    "                 lr, epochs, verbose=False, build=True, load_weights=False):\n",
    "        self.output_directory = output_directory\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if (verbose == True):\n",
    "                self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            if load_weights == True:\n",
    "                self.model.load_weights(self.output_directory\n",
    "                                        .replace('resnet_augment', 'resnet')\n",
    "                                        .replace('TSC_itr_augment_x_10', 'TSC_itr_10')\n",
    "                                        + '/model_init.hdf5')\n",
    "            else:\n",
    "                self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "        return\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        n_feature_maps = 64\n",
    "\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        # BLOCK 1\n",
    "\n",
    "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "        # BLOCK 2\n",
    "\n",
    "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # expand channels for the sum\n",
    "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "        # BLOCK 3\n",
    "\n",
    "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "        # no need to expand channels because they are equal\n",
    "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "        # FINAL\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "\n",
    "        file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "\n",
    "        self.callbacks = [reduce_lr, model_checkpoint]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val):\n",
    "        # if not tf.test.is_gpu_available:\n",
    "        #     print('error')\n",
    "        #     exit()\n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "        # batch_size = 64\n",
    "        # nb_epochs = 1500\n",
    "\n",
    "        # mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        hist = self.model.fit(x_train, y_train,\n",
    "                              batch_size=self.batch_size,\n",
    "                              epochs=self.epochs,\n",
    "                              verbose=self.verbose,\n",
    "                              validation_data=(x_val, y_val),\n",
    "                              callbacks=self.callbacks)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
    "        train_pred = self.model.predict(x_train, batch_size=self.batch_size)\n",
    "        test_pred = self.model.predict(x_val, batch_size=self.batch_size)\n",
    "\n",
    "        # y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
    "        #                       return_df_metrics=False)\n",
    "\n",
    "        # save predictions\n",
    "        # np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "        # y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return hist, train_pred, test_pred\n",
    "\n",
    "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
    "        start_time = time.time()\n",
    "        model_path = self.output_directory + 'best_model.hdf5'\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(x_test)\n",
    "        if return_df_metrics:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "            return df_metrics\n",
    "        else:\n",
    "            test_duration = time.time() - start_time\n",
    "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "            return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, seq_len, num_features = x_train.shape\n",
    "cls = Classifier_RESNET(output_directory='./', \n",
    "                        input_shape=(seq_len, num_features),\n",
    "                        nb_classes=2,\n",
    "                        batch_size=32,\n",
    "                        lr=1e-05,\n",
    "                        epochs=50,\n",
    "                        verbose=True,\n",
    "                        build=True,\n",
    "                        load_weights=False)\n",
    "\n",
    "hist, train_pred, val_pred = cls.fit(x_train, y_train, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss - MSE')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for additional metric\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, \n",
    "                 output_directory,\n",
    "                 input_shape,\n",
    "                 nb_classes, \n",
    "                 batch_size=64,\n",
    "                 lr=0.001,\n",
    "                 nb_epochs=1500,\n",
    "                 verbose=False,\n",
    "                 build=True,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41):\n",
    "\n",
    "        self.output_directory = output_directory\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if (verbose == True):\n",
    "                self.model.summary()\n",
    "#             self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > self.bottleneck_size:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(self.lr),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5,\n",
    "                                                      min_lr=1e-07)\n",
    "\n",
    "        file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "\n",
    "        self.callbacks = [reduce_lr, model_checkpoint]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val):\n",
    "#         if not tf.test.is_gpu_available:\n",
    "#             print('error no gpu')\n",
    "#             exit()\n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
    "        train_pred = self.model.predict(x_train, batch_size=self.batch_size)\n",
    "        val_pred = self.model.predict(x_val, batch_size=self.batch_size)\n",
    "\n",
    "#         y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
    "#                               return_df_metrics=False)\n",
    "\n",
    "        # save predictions\n",
    "#         np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "#         y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "#         df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        return hist, train_pred, val_pred\n",
    "\n",
    "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
    "        start_time = time.time()\n",
    "        model_path = self.output_directory + 'best_model.hdf5'\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "        if return_df_metrics:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
    "            return df_metrics\n",
    "        else:\n",
    "            test_duration = time.time() - start_time\n",
    "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
    "            return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH, base_lr, EPOCHS = 32, 1e-05, 50\n",
    "cls = Classifier_INCEPTION(output_directory='./', \n",
    "                    input_shape=(seq_len, num_features), \n",
    "                    nb_classes=2, \n",
    "                    batch_size=BATCH,\n",
    "                    lr=base_lr,\n",
    "                    nb_epochs=EPOCHS,\n",
    "                    verbose=True, \n",
    "                    build=True,\n",
    "                    nb_filters=32, \n",
    "                    use_residual=True, \n",
    "                    use_bottleneck=True, \n",
    "                    depth=6, \n",
    "                    kernel_size=41, \n",
    "#                     lr_scheduler=scheduler\n",
    "                          )\n",
    "hist, train_pred, val_pred = cls.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss - MSE')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for additional metric\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = cls.model.predict(x_test, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_true=y_train, y_score=train_pred)\n",
    "val_auc = roc_auc_score(y_true=y_val, y_score=val_pred)\n",
    "test_auc = roc_auc_score(y_true=y_test, y_score=test_pred)\n",
    "print(f\"train-auc: {train_auc}, val-auc: {val_auc}, test-auc: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "average_precision = average_precision_score(y_test, test_pred)\n",
    "precision, recall, thresholds = precision_recall_curve(np.argmax(y_test, axis=1), test_pred[:,1])\n",
    "\n",
    "line_kwargs = {\"drawstyle\": \"steps-post\"}\n",
    "# line_kwargs = {}\n",
    "line_kwargs[\"label\"] = (f\"AP = {average_precision:0.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel)\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for CNN Models\n",
    "    - extract past 1d/2d/7d etc. history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-v03.pkl', 'rb') as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "    \n",
    "print(all_data.shape)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for hourly data\n",
    "all_data = all_data[all_data.date.dt.minute==0].copy()\n",
    "\n",
    "# filter for daily data\n",
    "all_data = all_data[all_data.date.dt.hour==6]\n",
    "\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_col = 'label_1h'\n",
    "label_col = 'label_24h'\n",
    "\n",
    "label_df = all_data[['date', 'inverter', label_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_df = pd.read_csv('all_inverters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1  # in days\n",
    "base_features = [\n",
    "                'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)'\n",
    "               ]\n",
    "dfg = label_df.groupby('inverter')\n",
    "x, y = dict(), dict()\n",
    "for inverter, df in dfg:\n",
    "    features = ['IN.GMRX.CHAR.'+inverter+'.Active Power (kW)'] + base_features\n",
    "    columns = ['date'] + features\n",
    "    inv_df_i = inv_df[columns].copy()\n",
    "    inv_df_i['date'] = pd.to_datetime(inv_df_i[\"date\"])\n",
    "    inv_df_i.rename(columns={'IN.GMRX.CHAR.'+inverter+'.Active Power (kW)': 'power',\n",
    "                            'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)': 'temp1',\n",
    "                            'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)': 'rad1',\n",
    "                            'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)': 'temp2',\n",
    "                            'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)': 'rad2'}, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    x_inv, y_inv = [], []\n",
    "    for jj, row in df.iterrows():\n",
    "        end = row['date']\n",
    "        start = end - pd.Timedelta(window, 'D')\n",
    "        x_jj = inv_df_i[(inv_df_i.date >  start) & (inv_df_i.date <= end)][['power', 'temp1', 'rad1']].values\n",
    "        y_jj = row[label_col]\n",
    "        x_inv.append(x_jj)\n",
    "        y_inv.append(y_jj)\n",
    "    x_inv = np.stack(x_inv, axis=0)\n",
    "    y_inv = np.array(y_inv)\n",
    "    x[inverter] = x_inv\n",
    "    y[inverter] = y_inv\n",
    "    print(inverter, x_inv.shape, y_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (x, y, label_df)\n",
    "with open('inverter-data-cnn-daily.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (x, y, label_df)\n",
    "with open('inverter-data-cnn-v01.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
