{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_df = pd.read_csv('all_inverters.csv')\n",
    "# inv_df.head()\n",
    "\n",
    "target_codes = [7006, 3511, 7502, 7501, 3504, 6448, 1500, 7704]\n",
    "alarm_df = pd.read_csv('all_alarms.csv')\n",
    "alarm_df = alarm_df[alarm_df[\"Error Code\"].isin(target_codes)]\n",
    "alarm_df = alarm_df[(alarm_df.hod >= 6) & (alarm_df.hod <= 18)]  # original (6,17)\n",
    "print(alarm_df.shape)\n",
    "inverters = sorted(alarm_df[\"Controller Name\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-labels.pkl', 'rb') as handle:\n",
    "    inv_labels = pickle.load(handle)\n",
    "    \n",
    "label_df = {'inverters': [], 'positive': [], 'negative': []}\n",
    "for inv in inv_labels.keys():\n",
    "    x = inv_labels[inv]\n",
    "    y = dict(x['label'].value_counts())\n",
    "    label_df['inverters'].append(inv)\n",
    "    if 1 in y:\n",
    "        label_df['positive'].append(y[1])\n",
    "    else:\n",
    "        label_df['positive'].append(0)\n",
    "    label_df['negative'].append(y[0])\n",
    "label_df = pd.DataFrame(label_df)\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ratio\n",
    "mask = label_df['positive'] > 10\n",
    "total = label_df[mask][['positive', 'negative']].apply(np.sum, axis=0)\n",
    "100 * total['positive'] / (total['positive'] + total['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create rolling windows manually and extract features using tsfresh\n",
    "    - use ray to parallelize operations on each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tsfresh_features(df, colnames, ROLLING_WINDOWS):\n",
    "#     start_step = 1\n",
    "#     df[\"id\"] = 1  # tsfresh requirement\n",
    "#     for col in colnames:\n",
    "#         for window in ROLLING_WINDOWS:\n",
    "#             feats = tsfresh_features(df[[col, 'date', 'id']], window_size=window).reset_index(drop=True)\n",
    "#             df = pd.concat([df, feats], axis=1)\n",
    "#     return df    \n",
    "\n",
    "\n",
    "@ray.remote(num_returns=1)\n",
    "def extract_ts_features_(df, start, end, settings):\n",
    "    df_ = df[start:end].copy()\n",
    "    return extract_features(df_, \n",
    "                            column_id=\"id\", \n",
    "                            column_sort=\"date\",\n",
    "                            default_fc_parameters=settings,\n",
    "                            disable_progressbar=True,\n",
    "                           )\n",
    "\n",
    "\n",
    "def tsfresh_features(df, window_length, otype='spark'):\n",
    "    df_ = df.head(window_length)\n",
    "    settings = ComprehensiveFCParameters()\n",
    "#     settings = MinimalFCParameters()\n",
    "    \n",
    "    tic = time.time()\n",
    "    # returns a pandas DataFrame\n",
    "    features = extract_features(df_, \n",
    "                                column_id=\"id\", \n",
    "                                column_sort=\"date\", \n",
    "                                default_fc_parameters=settings,\n",
    "                                disable_progressbar=True,\n",
    "                               )\n",
    "    toc = time.time()\n",
    "    print(toc - tic)\n",
    "    num_features = features.shape[1]\n",
    "    n = df.shape[0]\n",
    "    print(\"Number of windows:\", n - window_length)\n",
    "    \n",
    "    # initialize with Nan values\n",
    "#     all_features = np.empty((n, num_features))\n",
    "#     all_features[:] = np.nan\n",
    "    all_features, dates = [], []\n",
    "    for ii in tqdm(range(n)):\n",
    "        if ii < window_length:\n",
    "            start, end = 0, ii+1\n",
    "        else:\n",
    "            start, end = ii - window_length + 1, ii+1\n",
    "        features = extract_ts_features_.remote(df, start, end, settings)\n",
    "        \n",
    "#         if otype == 'dask':\n",
    "#             # dask\n",
    "#             sd = dd.from_pandas(df_, npartitions=3)\n",
    "#             features = extract_features(sd, \n",
    "#                                         column_id=\"id\", \n",
    "#                                         column_sort=\"date\",\n",
    "#                                         default_fc_parameters=settings,\n",
    "#                                         disable_progressbar=True,\n",
    "#                                        )\n",
    "#         elif otype == 'spark':\n",
    "#             df_.rename(columns={'power': 'value'}, inplace=True)\n",
    "#             df_['kind'] = 'power'\n",
    "#             df_.to_csv('temp.csv', index=False)\n",
    "#             df_ = sparkl.read.csv('temp.csv', header=True)\n",
    "# #             df_.printSchema()\n",
    "# #             df_.show()\n",
    "# #             print(df_)\n",
    "#             df_grouped = df_.groupby([\"id\", \"kind\"])\n",
    "#             features = spark_feature_extraction_on_chunk(df_grouped, \n",
    "#                                                          column_id=\"id\", \n",
    "#                                                          column_kind=\"kind\",\n",
    "#                                                          column_sort=\"date\", \n",
    "#                                                          column_value=\"value\",\n",
    "#                                                          default_fc_parameters=MinimalFCParameters())\n",
    "#             features.printSchema()\n",
    "#             features.show() # does not work\n",
    "#             # pivot does not work\n",
    "# #             features = features.groupby(\"id\").pivot(\"variable\") #.sum(\"value\")\n",
    "#             print(features)\n",
    "#             sys.exit(\"HERE\")\n",
    "\n",
    "\n",
    "#         all_features[ii,:] = features\n",
    "        all_features.append(features)\n",
    "#         print(ii, df_.shape, features.shape)\n",
    "        dates.append(df.iloc[ii]['date'])\n",
    "    \n",
    "    new_features = ray.get(all_features)\n",
    "    return new_features, dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows = [x*12*24 for x in [1, 2, 3, 7, 14, 21, 30]]\n",
    "windows = [x*12*24 for x in [1]]\n",
    "TIMESTAMP_COL_NAME = 'date'\n",
    "data = []\n",
    "for inverter in inv_labels.keys():\n",
    "    x = inv_labels[inverter]\n",
    "    y = dict(x['label'].value_counts())\n",
    "    if 1 in y and y[1] > 10:\n",
    "        features = ['IN.GMRX.CHAR.'+inverter+'.Active Power (kW)', \n",
    "                'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)'\n",
    "               ]\n",
    "        columns = ['date'] + features\n",
    "        inv_df_i = inv_df[columns].copy()\n",
    "        inv_df_i['date'] = pd.to_datetime(inv_df_i[\"date\"])\n",
    "        inv_df_i.rename(columns={'IN.GMRX.CHAR.'+inverter+'.Active Power (kW)': 'power',\n",
    "                                'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)': 'temp1',\n",
    "                                'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)': 'rad1',\n",
    "                                'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)': 'temp2',\n",
    "                                'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)': 'rad2'}, inplace=True)\n",
    "        inv_df_i['hour'] = inv_df_i.date.dt.hour\n",
    "#         df_try = inv_df_i.head(10000)\n",
    "        # without tsfresh features - it takes 7 seconds\n",
    "        # df = inv_df_i[['power']].copy()\n",
    "        df = inv_df_i[['date', 'power']].copy()\n",
    "        # df = df.head(10000)\n",
    "        # df = df.reset_index(drop=False)\n",
    "        # df.rename(columns={'power': 'value'}, inplace=True)\n",
    "        df['id'] = 1\n",
    "        # df['kind'] = 'power'\n",
    "        print(df.columns)\n",
    "        tic = time.time()\n",
    "        extracted_features, dates = tsfresh_features(df, window_length=12*24)\n",
    "        # extracted_features = np.concatenate(extracted_features, axis=0 )\n",
    "        extracted_features = pd.concat(extracted_features, axis=0)\n",
    "        extracted_features['dates'] = dates\n",
    "        toc = time.time()\n",
    "        print(toc - tic)\n",
    "\n",
    "        \n",
    "        \n",
    "#         df_ = create_features(inv_df_i, colnames=['power', 'temp1', 'rad1'], ROLLING_WINDOWS=windows)\n",
    "#         toc = time.time()\n",
    "        sys.exit(toc-tic)\n",
    "        df_ = x.merge(df_, on='date', how='left')\n",
    "        y = df_['label'].value_counts()\n",
    "        print(inverter, x.shape[0], df_.shape[0], y[1], y[0])\n",
    "        data.append(df_)\n",
    "    else:\n",
    "        continue\n",
    "data = pd.concat(data, axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tsfresh.feature_extraction.feature_calculators import (\n",
    "    abs_energy, absolute_sum_of_changes, agg_autocorrelation, \n",
    "    agg_linear_trend, approximate_entropy, ar_coefficient, \n",
    "    augmented_dickey_fuller, autocorrelation, benford_correlation, \n",
    "    binned_entropy, c3, change_quantiles, cid_ce, count_above, \n",
    "    count_above_mean, count_below, count_below_mean, cwt_coefficients, \n",
    "    energy_ratio_by_chunks, fft_aggregated, fft_coefficient, first_location_of_maximum, \n",
    "    first_location_of_minimum, fourier_entropy, friedrich_coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh_functions = [\n",
    "#                      abs_energy, \n",
    "#                      absolute_sum_of_changes, \n",
    "#                      agg_autocorrelation, \n",
    "#                      agg_linear_trend, \n",
    "#                      approximate_entropy, \n",
    "#                      augmented_dickey_fuller, \n",
    "#                      autocorrelation, \n",
    "#                      benford_correlation, \n",
    "#                      binned_entropy, \n",
    "#                      c3, \n",
    "#                      change_quantiles, \n",
    "#                      cid_ce, \n",
    "#                      count_above, count_below, energy_ratio_by_chunks,\n",
    "#                      ar_coefficient, cwt_coefficients, fft_coefficient, friedrich_coefficients,\n",
    "#                      count_above_mean, \n",
    "#                      count_below_mean, \n",
    "#                      fft_aggregated, \n",
    "#                      first_location_of_maximum, \n",
    "#                      first_location_of_minimum, \n",
    "#                      fourier_entropy, \n",
    "                    ]\n",
    "\n",
    "# @jit(nopython=False)\n",
    "def abs_energy(x):\n",
    "    return np.dot(x, x)\n",
    "\n",
    "ts_funs = [abs_energy]\n",
    "# np_funs = [np.mean, np.std, 'min', 'max', 'median', 'skew', 'kurt', 'sum']\n",
    "np_funs = [np.mean, np.std, 'min', 'max', 'median']\n",
    "\n",
    "def rolling_features(df, start_step, window_size, funcs):\n",
    "#     df = df.reset_index(drop=False)\n",
    "#     df.rename(columns={'power': 'value'}, inplace=True)\n",
    "#     df['id'] = 1\n",
    "#     df['kind'] = 'power'\n",
    "#     df.to_csv('temp.csv', index=False)\n",
    "#     df = sparkl.read.csv('temp.csv', header=True)\n",
    "#     df.printSchema()\n",
    "#     df.show()\n",
    "#     sys.exit(\"Here\")\n",
    "    features = df.shift(start_step).rolling(window_size, min_periods=window_size).agg(funcs)\n",
    "    features.columns = [\"{}_{}{}\".format(x[0], x[1], str(int(window_size/12/24))+'d') for x in features.columns]\n",
    "    return features\n",
    "\n",
    "def create_features(df, colnames, ROLLING_WINDOWS):\n",
    "    # Feature engineering\n",
    "    df[\"day\"] = df[TIMESTAMP_COL_NAME].apply(lambda x: x.day)\n",
    "    df[\"dayofweek\"] = df[TIMESTAMP_COL_NAME].apply(lambda x: x.dayofweek)\n",
    "    df[\"weekofyear\"] = df[TIMESTAMP_COL_NAME].apply(lambda x: x.isocalendar()[1])\n",
    "    df[\"month\"] = df[TIMESTAMP_COL_NAME].apply(lambda x: x.month)\n",
    "    \n",
    "    # assume each record is the first day of forecast period so shift rolling calcs back by 1\n",
    "    start_step = 1 \n",
    "    for col in colnames:\n",
    "        for window in ROLLING_WINDOWS:\n",
    "            feats = rolling_features(df[[col]], \n",
    "                                     start_step=1, \n",
    "                                     window_size=window, \n",
    "                                     funcs=np_funs).reset_index(drop=True)\n",
    "            df = pd.concat([df, feats], axis=1)\n",
    "    return df\n",
    "\n",
    "windows = [x*12*24 for x in [1, 2, 3, 7, 14, 21, 30]]\n",
    "TIMESTAMP_COL_NAME = 'date'\n",
    "data = []\n",
    "for inverter in inv_labels.keys():\n",
    "    x = inv_labels[inverter]\n",
    "    y = dict(x['label'].value_counts())\n",
    "    if True in y and y[True] > 10:\n",
    "        features = ['IN.GMRX.CHAR.'+inverter+'.Active Power (kW)', \n",
    "                'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)',\n",
    "                'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)'\n",
    "               ]\n",
    "        columns = ['date'] + features\n",
    "        inv_df_i = inv_df[columns].copy()\n",
    "        inv_df_i['date'] = pd.to_datetime(inv_df_i[\"date\"])\n",
    "        inv_df_i.rename(columns={'IN.GMRX.CHAR.'+inverter+'.Active Power (kW)': 'power',\n",
    "                                'IN.GMRX.CHAR.WS-20 MW.Module Temperature (°C)': 'temp1',\n",
    "                                'IN.GMRX.CHAR.WS-20 MW.POA Irradiance (w/m²)': 'rad1',\n",
    "                                'IN.GMRX.CHAR.WS-5 MW.Module Temperature (°C)': 'temp2',\n",
    "                                'IN.GMRX.CHAR.WS-5 MW.POA Irradiance (w/m²)': 'rad2'}, inplace=True)\n",
    "        inv_df_i['hour'] = inv_df_i.date.dt.hour\n",
    "        tic = time.time()\n",
    "        df_ = create_features(inv_df_i, colnames=['power', 'temp1', 'rad1'], ROLLING_WINDOWS=windows)\n",
    "        toc = time.time()\n",
    "#         print(toc - tic)\n",
    "#         sys.exit()\n",
    "        df_ = x.merge(df_, on='date', how='left')\n",
    "        df_['inverter'] = inverter\n",
    "        y = df_['label'].value_counts()\n",
    "        print(inverter, x.shape[0], df_.shape[0], y[1], y[0], toc-tic)\n",
    "        data.append(df_)\n",
    "    else:\n",
    "        continue\n",
    "data = pd.concat(data, axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverter-data-v02.pkl', 'wb') as handle:\n",
    "    pickle.dump(data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(data)//1024//1024//1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = inv_df_i[['power', 'date']].copy()\n",
    "df[\"id\"] = 1\n",
    "df = df.head(80000)\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "# try dask\n",
    "# df = dd.from_pandas(df, npartitions=2)\n",
    "\n",
    "tic = time.time()\n",
    "window = 12*24\n",
    "df_rolled = roll_time_series(df, \n",
    "                             column_id='id', \n",
    "                             column_sort=\"index\", \n",
    "                             rolling_direction=1, \n",
    "                             min_timeshift = window - 1, \n",
    "                             max_timeshift = window - 1,\n",
    "                             disable_progressbar=False,\n",
    "                             n_jobs = 5,\n",
    "                            )\n",
    "\n",
    "toc = time.time()\n",
    "print(toc - tic)\n",
    "df_rolled.drop(columns=['index'], inplace=True)\n",
    "settings = MinimalFCParameters()\n",
    "features = extract_features(df_rolled, \n",
    "                            column_id=\"id\", \n",
    "                            column_sort=\"date\", \n",
    "                            default_fc_parameters=settings,\n",
    "                            disable_progressbar=False,\n",
    "                           )\n",
    "tac = time.time()\n",
    "print(tac - toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkl = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inv_df_i[['date', 'power']].copy()\n",
    "df.rename(columns={'power': 'value'}, inplace=True)\n",
    "df['id'] = 1\n",
    "df['kind'] = 'power'\n",
    "df.to_csv('temp.csv', index=False)\n",
    "df = sparkl.read.csv('temp.csv', header=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.convenience.bindings import spark_feature_extraction_on_chunk\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext(\"local\", \"First App\")\n",
    "\n",
    "#Create PySpark SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[1]\") \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .getOrCreate()\n",
    "#Create PySpark DataFrame from Pandas\n",
    "# sparkDF=sparkl.createDataFrame(df) \n",
    "df_grouped = df.groupby([\"id\", \"kind\"])\n",
    "features = spark_feature_extraction_on_chunk(df_grouped, column_id=\"id\", column_kind=\"kind\",\n",
    "                                             column_sort=\"date\", column_value=\"value\",\n",
    "                                             default_fc_parameters=MinimalFCParameters())\n",
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.show()\n",
    "# features = features.groupby(\"id\").pivot(\"variable\").sum(\"value\")\n",
    "features.write.csv(\"temp_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inv_df_i[['date', 'power']].copy()\n",
    "df = df.reset_index(drop=False)\n",
    "df.rename(columns={'power': 'value'}, inplace=True)\n",
    "df['id'] = 1\n",
    "df['kind'] = 'power'\n",
    "df.to_parquet('temp.pqt', index=False)\n",
    "df = dd.read_parquet('temp.pqt', header=True)\n",
    "\n",
    "window = 12*24\n",
    "df_rolled = roll_time_series(df, \n",
    "                             column_id='id', \n",
    "                             column_sort=\"index\", \n",
    "                             rolling_direction=1, \n",
    "                             min_timeshift = window - 1, \n",
    "                             max_timeshift = window - 1,\n",
    "                             disable_progressbar=False,\n",
    "                             n_jobs = 5,\n",
    "                            )\n",
    "\n",
    "\n",
    "# from tsfresh.convenience.bindings import dask_feature_extraction_on_chunk\n",
    "# df_grouped = df.groupby([\"id\", \"kind\"])\n",
    "\n",
    "# features = dask_feature_extraction_on_chunk(df_grouped,\n",
    "#                                             column_id=\"id\",\n",
    "#                                             column_kind=\"kind\",\n",
    "#                                             column_sort=\"date\",\n",
    "#                                             column_value=\"value\")\n",
    "# features = features.categorize(columns=[\"variable\"])\n",
    "# features = features.reset_index(drop=True) \\\n",
    "#             .pivot_table(index=\"id\", columns=\"variable\", values=\"value\", aggfunc=\"mean\")\n",
    "# features.to_csv(\"temp_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled[df_rolled.id==(1,288)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = inv_df_i[['power', 'date']].copy()\n",
    "df[\"id\"] = 1\n",
    "df = df.head(100)\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "df_rolled = roll_time_series(df, column_id='id', column_sort=\"index\", \n",
    "                             rolling_direction=1, \n",
    "                             min_timeshift = 9, \n",
    "                             max_timeshift = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled[df_rolled.id==(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolled.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rolled.drop(columns=['index'], inplace=True)\n",
    "settings = MinimalFCParameters()\n",
    "features = extract_features(df_rolled, \n",
    "                            column_id=\"id\", \n",
    "                            column_sort=\"date\", \n",
    "                            default_fc_parameters=settings,\n",
    "                            disable_progressbar=False,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
