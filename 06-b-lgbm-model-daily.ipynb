{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building for Hourly/Daily Predictions\n",
    "\n",
    "    - labels are created separately (see 04-label-generation.ipynb)\n",
    "    - labels are for alarms generated in the next 5 minutes, next 1 hour and next 1 day\n",
    "    - data points can be at 5 minutes interval, hourly level and day lavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-v03.pkl', 'rb') as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "    \n",
    "print(all_data.shape)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for hourly data\n",
    "all_data = all_data[all_data.date.dt.minute==0].copy()\n",
    "\n",
    "# filter for daily data\n",
    "all_data = all_data[all_data.date.dt.hour==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(columns=['label', 'label_1h', 'inverter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['hour', \"day\", \"dayofweek\", \"weekofyear\", \"month\"]\n",
    "for cat in categoricals:\n",
    "    all_data[cat] = all_data[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'label_24h'\n",
    "all_data[label_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = all_data[label_col].value_counts()\n",
    "print(f\"Class ratio: {100 * total[1] / (total[0] + total[1]):.3f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_type = 'random'\n",
    "train_days, eval_days = 60, 60\n",
    "dmin, dmax = all_data.date.min(), all_data.date.max()\n",
    "train_end = dmax - pd.Timedelta(eval_days, 'D')\n",
    "train_start = train_end - pd.Timedelta(train_days, 'D')\n",
    "print(dmin, dmax, train_start, train_end)\n",
    "\n",
    "if split_type == 'random':\n",
    "    # random split\n",
    "    train, test = train_test_split(all_data, train_size=0.8, random_state=100)\n",
    "\n",
    "else:\n",
    "    # train, test = all_data[all_data.date < train_cutoff], all_data[all_data.date >= train_cutoff]\n",
    "    train, test = all_data[(all_data.date >= train_start) & (all_data.date < train_end)], all_data[all_data.date >= train_end]\n",
    "\n",
    "train, valid = train_test_split(train, train_size=0.8, random_state=100)\n",
    "print(train.shape, valid.shape, test.shape)\n",
    "\n",
    "train[label_col].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[label_col].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['yearmo'] = all_data['date'].apply(lambda x: f\"{x.year}{x.month:02d}\")\n",
    "all_data['yearmo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = all_data[['yearmo', label_col]].groupby(['yearmo']).agg(np.mean).reset_index()\n",
    "dft.plot(x='yearmo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(columns=['yearmo'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, df_train, df_valid, categoricals, fixed_params, param_set={}, verbose_eval=50):\n",
    "        self.categoricals = categoricals\n",
    "        self.fixed_params = fixed_params\n",
    "        self.param_set = param_set\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.dtrain = lgb.Dataset(\n",
    "            df_train.drop([label_col], axis=1),\n",
    "            label = df_train[label_col],\n",
    "            categorical_feature=self.categoricals,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.dvalid = lgb.Dataset(\n",
    "            df_valid.drop([label_col], axis=1),\n",
    "            label = df_valid[label_col],\n",
    "            categorical_feature=self.categoricals,\n",
    "            reference=self.dtrain,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.default_ranges = {\n",
    "            \"num_leaves\":(2, 256),\n",
    "            \"min_data_in_leaf\":(5, 100),\n",
    "            \"learning_rate\":(1e-3, 1e-1),\n",
    "            \"feature_fraction\":(0.4, 1.0),\n",
    "            \"bagging_freq\":(1, 7),\n",
    "            \"bagging_fraction\":(0.4, 1.0)\n",
    "        }\n",
    "        \n",
    "    def get_params(self, trial):\n",
    "        param_funcs = {\n",
    "            \"num_leaves\":trial.suggest_int,\n",
    "            \"min_data_in_leaf\":trial.suggest_int,\n",
    "            \"learning_rate\":trial.suggest_loguniform,\n",
    "            \"feature_fraction\":trial.suggest_float,\n",
    "            \"bagging_freq\":trial.suggest_int,\n",
    "            \"bagging_fraction\":trial.suggest_float\n",
    "        }\n",
    "        params = {}\n",
    "        for param, rng in self.param_set.items():\n",
    "            if rng is None:\n",
    "                default_rng = self.default_ranges[param]\n",
    "                params[param] = param_funcs[param](param, default_rng[0], default_rng[1])\n",
    "            else:\n",
    "                params[param] = param_funcs[param](param, rng[0], rng[1])\n",
    "\n",
    "        params.update(self.fixed_params)\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        params = self.get_params(trial)\n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            self.dtrain,\n",
    "            valid_sets=[self.dvalid],\n",
    "            verbose_eval=self.verbose_eval\n",
    "        )\n",
    "        # get best value of objective\n",
    "        valid_0 = bst.best_score['valid_0']\n",
    "        score = valid_0[list(valid_0)[0]]\n",
    "        \n",
    "        trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "        trial.set_user_attr('features', self.dtrain.feature_name)\n",
    "        trial.set_user_attr('importance', bst.feature_importance().tolist())\n",
    "        \n",
    "        return score\n",
    "\n",
    "class EarlyStoppingExceeded(optuna.exceptions.OptunaError):\n",
    "    pass\n",
    "\n",
    "class EarlyStoppingCallback(object):\n",
    "    # from https://github.com/optuna/optuna/issues/1001#issuecomment-596478792\n",
    "    \n",
    "    def __init__(self, early_stopping_rounds, min_delta):\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.min_delta = min_delta\n",
    "        self.early_stopping_count = 0\n",
    "        self.best_score = None\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        if self.best_score == None:\n",
    "            self.best_score = study.best_value\n",
    "\n",
    "        if study.best_value < self.best_score - self.min_delta:\n",
    "            self.best_score = study.best_value\n",
    "            self.early_stopping_count = 0\n",
    "        else:\n",
    "            if self.early_stopping_count > self.early_stopping_rounds:\n",
    "                self.early_stopping_count = 0\n",
    "                best_score = None\n",
    "                raise EarlyStoppingExceeded()\n",
    "            else:\n",
    "                self.early_stopping_count += 1\n",
    "        return\n",
    "    \n",
    "\n",
    "def tune_model(df_train, df_valid, categoricals, fixed_params, param_set, n_trials=50, verbose_eval=50, show_progress=True, early_stop_callback=None, tpe_mode=\"independent\"):\n",
    "    multivariate_flag = True if tpe_mode == \"multivariate\" else False\n",
    "    sampler = optuna.samplers.TPESampler(multivariate=multivariate_flag)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    callbacks = None\n",
    "    if early_stop_callback is not None:\n",
    "        callbacks = [early_stop_callback]\n",
    "    else:\n",
    "        callbacks = []\n",
    "    try:\n",
    "        study.optimize(\n",
    "            Objective(\n",
    "                df_train=df_train,\n",
    "                df_valid=df_valid,\n",
    "                categoricals=categoricals,\n",
    "                fixed_params=fixed_params,\n",
    "                param_set = param_set,\n",
    "                verbose_eval=verbose_eval\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=show_progress,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except EarlyStoppingExceeded:\n",
    "        print(f'EarlyStopping Exceeded: No new best scores on iters {early_stop_callback.early_stopping_rounds}')\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = 'binary'\n",
    "num_rounds = 1000\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "print(\"Tune hyperparameters...\")\n",
    "param_set = {\n",
    "    \"num_leaves\":None, \n",
    "    \"min_data_in_leaf\":None, \n",
    "    \"learning_rate\":None, \n",
    "    \"feature_fraction\":None,\n",
    "    \"bagging_freq\":None, \n",
    "    \"bagging_fraction\":None\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    \"objective\":obj_func,\n",
    "    \"metric\":[obj_func, \"auc\"],\n",
    "    \"num_rounds\":num_rounds,\n",
    "    \"early_stopping_rounds\":early_stopping_rounds,\n",
    "    \"first_metric_only\":True,\n",
    "    \"force_row_wise\":True,\n",
    "    \"feature_pre_filter\":False,\n",
    "    \"verbose\":1,\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(10, 0.001)\n",
    "\n",
    "study = tune_model(\n",
    "                    train.drop(columns=[\"date\"]),\n",
    "                    valid.drop(columns=[\"date\"]),\n",
    "                    categoricals, fixed_params, param_set, n_trials=100, verbose_eval=0,\n",
    "                    show_progress=False, early_stop_callback=early_stopping,\n",
    "                )\n",
    "\n",
    "print(\"Saving best model parameters...\")\n",
    "best_params = {k: [v] for (k,v) in study.best_params.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_params_24h.pkl', 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters:', best_params)\n",
    "num_rounds = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "\n",
    "fixed_params[\"num_rounds\"] = num_rounds\n",
    "# fixed_params[\"early_stopping_rounds\"] = 0\n",
    "params = study.best_params.copy()\n",
    "\n",
    "params.update(fixed_params)\n",
    "del params[\"early_stopping_rounds\"] # = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['verbose'] = 1\n",
    "params['metric'] = ['binary', 'auc']\n",
    "# params['is_unbalance'] = True\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           num_leaves=params['num_leaves'], \n",
    "                           min_data_in_leaf=params['min_data_in_leaf'],\n",
    "                           learning_rate=params['learning_rate'],\n",
    "                           feature_fraction=params['feature_fraction'],\n",
    "                           bagging_freq=params['bagging_freq'],\n",
    "                           bagging_fraction=params['bagging_fraction'],\n",
    "                           objective='binary',\n",
    "                           metric=params['metric'],\n",
    "                           num_rounds=params['num_rounds'],\n",
    "#                            is_unbalance=params['is_unbalance']\n",
    "                          )\n",
    "x_train, y_train = train.drop(columns=[label_col, \"date\"]), train[label_col]\n",
    "x_val, y_val = valid.drop(columns=[label_col, \"date\"]), valid[label_col]\n",
    "\n",
    "model.fit(X=x_train, y=y_train, \n",
    "          eval_set=[(x_val, y_val)],\n",
    "          eval_names=['eval']\n",
    "         )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = test.drop(columns=[label_col, \"date\"]), test[label_col]\n",
    "\n",
    "train_pred = model.predict_proba(x_train)\n",
    "val_pred = model.predict_proba(x_val)\n",
    "test_pred = model.predict_proba(x_test)\n",
    "train_pred.shape, val_pred.shape, test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_, model.importance_type, model.classes_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(y_true=y_train, y_score=train_pred[:,1])\n",
    "val_auc = roc_auc_score(y_true=y_val, y_score=val_pred[:,1])\n",
    "test_auc = roc_auc_score(y_true=y_test, y_score=test_pred[:,1])\n",
    "print(f\"train-auc: {train_auc}, val-auc: {val_auc}, test-auc: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "feature_imp = pd.DataFrame({'Value':model.feature_importances_,\n",
    "                            'Feature':train.drop(columns=[label_col, \"date\"]).columns})\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1.5)\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                    ascending=False)[0:num])\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances-01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve for the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(y_test, test_pred[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_pred[:,1])\n",
    "# disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "# disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "#                    'AP={0:0.2f}'.format(average_precision))\n",
    "line_kwargs1 = {\"drawstyle\": \"steps-post\", 'label': 'precision'}\n",
    "line_kwargs2 = {\"drawstyle\": \"steps-post\", 'label': 'threshold'}\n",
    "# line_kwargs = {}\n",
    "# line_kwargs[\"label\"] = ('precision', 'threshold')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs1)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs2)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel, title=f\"Average Precision = {average_precision:0.2f}\")\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve for the Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(y_val, val_pred[:,1])\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_pred[:,1])\n",
    "line_kwargs1 = {\"drawstyle\": \"steps-post\", 'label': 'precision'}\n",
    "line_kwargs2 = {\"drawstyle\": \"steps-post\", 'label': 'threshold'}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs1)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs2)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel, title=f\"Average Precision = {average_precision:0.2f}\")\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data['date']=='2020-01-30 12:25:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.date.min(), all_data.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = all_data.groupby('inverter')\n",
    "for inv, df in dfg:\n",
    "    print(inv)\n",
    "    print(df)\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.date.dt.minute==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.label_1h.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label_1h.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data[all_data.date.dt.hour==6]\n",
    "df.label_24h.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.dt.hour.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.min(), df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "features = ['power_max8640', 'power_max6048', 'power_max4032', 'power_max2016', 'power_max864', 'power_max576']\n",
    "\n",
    "for col in features:\n",
    "    df = all_data[[col, 'label_1h']]\n",
    "    print(df.groupby('label_1h').agg(np.mean))\n",
    "\n",
    "    data1 = df[df.label_1h==1][col]\n",
    "    data2 = df[df.label_1h==0][col]\n",
    "\n",
    "    density1 = gaussian_kde(data1)\n",
    "    density2 = gaussian_kde(data2)\n",
    "\n",
    "    xs = np.linspace(300, 500, 100)\n",
    "    density1.covariance_factor = lambda : .25\n",
    "    density1._compute_covariance()\n",
    "\n",
    "    density2.covariance_factor = lambda : .25\n",
    "    density2._compute_covariance()\n",
    "    plt.plot(xs, density1(xs), label='positive')\n",
    "    plt.plot(xs, density2(xs), label='negative')\n",
    "    plt.title(f'Density plot: {col}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['power_std8640', 'power_mean8640']\n",
    "\n",
    "for col in features:\n",
    "    df = all_data[[col, 'label_1h']]\n",
    "    print(df.groupby('label_1h').agg(np.mean))\n",
    "\n",
    "    data1 = df[df.label_1h==1][col]\n",
    "    data2 = df[df.label_1h==0][col]\n",
    "\n",
    "    density1 = gaussian_kde(data1)\n",
    "    density2 = gaussian_kde(data2)\n",
    "\n",
    "    xs = np.linspace(0, 200, 200)\n",
    "    density1.covariance_factor = lambda : .25\n",
    "    density1._compute_covariance()\n",
    "\n",
    "    density2.covariance_factor = lambda : .25\n",
    "    density2._compute_covariance()\n",
    "    plt.plot(xs, density1(xs), label='positive')\n",
    "    plt.plot(xs, density2(xs), label='negative')\n",
    "    plt.title(f'Density plot: {col}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train.sample(n=100000, random_state=100)\n",
    "# X, y = df.drop(columns=['label_1h', 'date']), df['label_1h']\n",
    "label_col = 'label_24h'\n",
    "X, y = train.drop(columns=[label_col, 'date']), train[label_col]\n",
    "\n",
    "clf = IsolationForest(random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = clf.predict(train.drop(columns=[label_col, 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'pred': train_pred, 'y': train[label_col]})\n",
    "res['pred'] = res['pred'].map({1: 0, -1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(res['y'], res['pred'], rownames=['actual'], colnames=['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict(test.drop(columns=[label_col, 'date']))\n",
    "res = pd.DataFrame({'pred': test_pred, 'y': test[label_col]})\n",
    "res['pred'] = res['pred'].map({1: 0, -1: 1})\n",
    "pd.crosstab(res['y'], res['pred'], rownames=['actual'], colnames=['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(res['y'], res['pred'], target_names=['Normal', 'Faulty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "113/(113 + 1086)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hyper-parameters*\n",
    "\n",
    "- kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’}\n",
    "- degree of the poly kernel, 1, 2, 3, ...\n",
    "- gamma: scale, auto or float\n",
    "- coef0: independent term in kernel function, for 'poly' and 'sigmoid'\n",
    "- nu: upper bound on the fraction of training errors and lower bound on the fraction of support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False\n",
    "\n",
    "X, y = train.drop(columns=[label_col, 'date']), train[label_col]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize:\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    clf = OneClassSVM(gamma='auto').fit(X_scaled)\n",
    "    train_pred = clf.predict(X_scaled)\n",
    "    test_pred = clf.predict(scaler.transform(test.drop(columns=[label_col, 'date'])))\n",
    "    test_score = clf.score_samples(scaler.transform(test.drop(columns=[label_col, 'date'])))\n",
    "\n",
    "else:\n",
    "    # clf = OneClassSVM(gamma='scale').fit(X)\n",
    "    clf = OneClassSVM(gamma='auto').fit(X)\n",
    "    train_pred = clf.predict(X)\n",
    "    test_pred = clf.predict(test.drop(columns=[label_col, 'date']))\n",
    "    test_score = clf.score_samples(test.drop(columns=[label_col, 'date']))\n",
    "    \n",
    "res_train = pd.DataFrame({'pred': train_pred, 'y': train[label_col]})\n",
    "# 1-class SVM prediction in {-1, +1}, -1 means outlier\n",
    "res_train['pred'] = res_train['pred'].map({1: 0, -1: 1})\n",
    "pd.crosstab(res_train['y'], res_train['pred'], rownames=['actual'], colnames=['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'pred': test_pred, 'y': test[label_col]})\n",
    "res['pred'] = res['pred'].map({1: 0, -1: 1})\n",
    "pd.crosstab(res['y'], res['pred'], rownames=['actual'], colnames=['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(res['y'], res['pred'], target_names=['Normal', 'Faulty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = clf.score_samples(test.drop(columns=[label_col, 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_label = 1\n",
    "average_precision = average_precision_score(test[label_col], test_score)\n",
    "precision, recall, thresholds = precision_recall_curve(test[label_col], test_score)\n",
    "line_kwargs1 = {\"drawstyle\": \"steps-post\", 'label': 'precision'}\n",
    "line_kwargs2 = {\"drawstyle\": \"steps-post\", 'label': 'threshold'}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set(font_scale = 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, **line_kwargs1)\n",
    "ax.plot(recall[:-1], thresholds, **line_kwargs2)\n",
    "info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                  if pos_label is not None else \"\")\n",
    "xlabel = \"Recall\" + info_pos_label\n",
    "ylabel = \"Precision\" + info_pos_label\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel, title=f\"Average Precision = {average_precision:0.2f}\")\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.min(), test_score.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = clf.score_samples(X)\n",
    "train_score.min(), train_score.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = np.expand_dims(train_score, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(random_state=0).fit(train_score, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_train_score = clf_lr.predict_proba(train_score)\n",
    "calibrated_train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_train_score[:,1].min(), calibrated_train_score[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.expand_dims(test_score, -1)\n",
    "calibrated_test_score = clf_lr.predict_proba(test_score)\n",
    "calibrated_test_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_test_score[:,1].min(), calibrated_test_score[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(test_y, calibrated_test_score[:,1], n_bins=10, normalize=True)\n",
    "\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_clf = OneClassSVM(gamma='auto')\n",
    "# calibrated_clf = CalibratedClassifierCV(base_estimator=base_clf, cv=3)\n",
    "# calibrated_clf.fit(X, y)\n",
    "\n",
    "# calibrator = CalibratedClassifierCV(clf, cv='prefit')\n",
    "# calibrator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = test.drop(columns=[label_col, 'date']), test[label_col]\n",
    "\n",
    "# predict probabilities\n",
    "probs = clf.decision_function(test_x)\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(test_y, probs, n_bins=10, normalize=True)\n",
    "\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import sklearn\n",
    "# from sklearn import datasets\n",
    "# def objective(trial):\n",
    "#     iris = sklearn.datasets.load_iris()\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
    "#     max_depth = int(trial.suggest_loguniform('max_depth', 1, 32))\n",
    "#     clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "#     return sklearn.model_selection.cross_val_score(clf, iris.data, iris.target, \n",
    "#        n_jobs=-1, cv=3).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
