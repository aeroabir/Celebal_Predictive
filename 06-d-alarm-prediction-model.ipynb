{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Alarm Type\n",
    "\n",
    "    - given that there is an alarm in the next T hours\n",
    "    - there could be multiple alarms with more than one count for each alarm\n",
    "    - simplified to predicting multiple binary outputs\n",
    "    - for alarm type x the label is 1 if the count is greater than 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_codes = [7006, 3511, 7502, 7501, 3504, 6448, 1500, 7704]\n",
    "\n",
    "label_df = pd.read_csv('multiclass_labels.csv')\n",
    "label_df['date'] = pd.to_datetime(label_df['date'])\n",
    "label_df = label_df.drop(['only_date', 'label', 'label_1h', 'label_24h', 'alarm_24h_concat'] + ['count_'+str(t) for t in target_codes], axis=1)\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverter-data-v03.pkl', 'rb') as handle:\n",
    "    all_data = pickle.load(handle)\n",
    "    \n",
    "print(all_data.shape)\n",
    "all_data.dropna(inplace=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = label_df.merge(all_data, on=['date', 'inverter'], how='left')\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dropna(inplace=True)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop(['inverter', 'label', 'label_1h', 'label_24h'], axis=1)\n",
    "# Note: date is dropped later\n",
    "categoricals = ['hour', \"day\", \"dayofweek\", \"weekofyear\", \"month\"]\n",
    "for cat in categoricals:\n",
    "    all_data[cat] = all_data[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, df_train, df_valid, categoricals, fixed_params, param_set={}, verbose_eval=50):\n",
    "        self.categoricals = categoricals\n",
    "        self.fixed_params = fixed_params\n",
    "        self.param_set = param_set\n",
    "        self.verbose_eval = verbose_eval\n",
    "        self.dtrain = lgb.Dataset(\n",
    "            df_train.drop([label_col], axis=1),\n",
    "            label = df_train[label_col],\n",
    "            categorical_feature=self.categoricals,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.dvalid = lgb.Dataset(\n",
    "            df_valid.drop([label_col], axis=1),\n",
    "            label = df_valid[label_col],\n",
    "            categorical_feature=self.categoricals,\n",
    "            reference=self.dtrain,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        self.default_ranges = {\n",
    "            \"num_leaves\":(2, 256),\n",
    "            \"min_data_in_leaf\":(5, 100),\n",
    "            \"learning_rate\":(1e-3, 1e-1),\n",
    "            \"feature_fraction\":(0.4, 1.0),\n",
    "            \"bagging_freq\":(1, 7),\n",
    "            \"bagging_fraction\":(0.4, 1.0)\n",
    "        }\n",
    "        \n",
    "    def get_params(self, trial):\n",
    "        param_funcs = {\n",
    "            \"num_leaves\":trial.suggest_int,\n",
    "            \"min_data_in_leaf\":trial.suggest_int,\n",
    "            \"learning_rate\":trial.suggest_loguniform,\n",
    "            \"feature_fraction\":trial.suggest_float,\n",
    "            \"bagging_freq\":trial.suggest_int,\n",
    "            \"bagging_fraction\":trial.suggest_float\n",
    "        }\n",
    "        params = {}\n",
    "        for param, rng in self.param_set.items():\n",
    "            if rng is None:\n",
    "                default_rng = self.default_ranges[param]\n",
    "                params[param] = param_funcs[param](param, default_rng[0], default_rng[1])\n",
    "            else:\n",
    "                params[param] = param_funcs[param](param, rng[0], rng[1])\n",
    "\n",
    "        params.update(self.fixed_params)\n",
    "        return params\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        params = self.get_params(trial)\n",
    "        bst = lgb.train(\n",
    "            params,\n",
    "            self.dtrain,\n",
    "            valid_sets=[self.dvalid],\n",
    "            verbose_eval=self.verbose_eval\n",
    "        )\n",
    "        # get best value of objective\n",
    "        valid_0 = bst.best_score['valid_0']\n",
    "        score = valid_0[list(valid_0)[0]]\n",
    "        \n",
    "        trial.set_user_attr('best_iteration', bst.best_iteration)\n",
    "        trial.set_user_attr('features', self.dtrain.feature_name)\n",
    "        trial.set_user_attr('importance', bst.feature_importance().tolist())\n",
    "        \n",
    "        return score\n",
    "\n",
    "class EarlyStoppingExceeded(optuna.exceptions.OptunaError):\n",
    "    pass\n",
    "\n",
    "class EarlyStoppingCallback(object):\n",
    "    # from https://github.com/optuna/optuna/issues/1001#issuecomment-596478792\n",
    "    \n",
    "    def __init__(self, early_stopping_rounds, min_delta):\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.min_delta = min_delta\n",
    "        self.early_stopping_count = 0\n",
    "        self.best_score = None\n",
    "    \n",
    "    def __call__(self, study, trial):\n",
    "        if self.best_score == None:\n",
    "            self.best_score = study.best_value\n",
    "\n",
    "        if study.best_value < self.best_score - self.min_delta:\n",
    "            self.best_score = study.best_value\n",
    "            self.early_stopping_count = 0\n",
    "        else:\n",
    "            if self.early_stopping_count > self.early_stopping_rounds:\n",
    "                self.early_stopping_count = 0\n",
    "                best_score = None\n",
    "                raise EarlyStoppingExceeded()\n",
    "            else:\n",
    "                self.early_stopping_count += 1\n",
    "        return\n",
    "    \n",
    "\n",
    "def tune_model(df_train, df_valid, categoricals, fixed_params, param_set, n_trials=50, verbose_eval=50, show_progress=True, early_stop_callback=None, tpe_mode=\"independent\"):\n",
    "    multivariate_flag = True if tpe_mode == \"multivariate\" else False\n",
    "    sampler = optuna.samplers.TPESampler(multivariate=multivariate_flag)\n",
    "    study = optuna.create_study(sampler=sampler)\n",
    "    callbacks = None\n",
    "    if early_stop_callback is not None:\n",
    "        callbacks = [early_stop_callback]\n",
    "    else:\n",
    "        callbacks = []\n",
    "    try:\n",
    "        study.optimize(\n",
    "            Objective(\n",
    "                df_train=df_train,\n",
    "                df_valid=df_valid,\n",
    "                categoricals=categoricals,\n",
    "                fixed_params=fixed_params,\n",
    "                param_set = param_set,\n",
    "                verbose_eval=verbose_eval\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=show_progress,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except EarlyStoppingExceeded:\n",
    "        print(f'EarlyStopping Exceeded: No new best scores on iters {early_stop_callback.early_stopping_rounds}')\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [7501]:\n",
    "# for t in target_codes:\n",
    "    print(f\"Modeling for {t} label\")\n",
    "    drop_labels = ['label_'+str(tc) for tc in target_codes if tc != t]\n",
    "    label_col = 'label_' + str(t)\n",
    "    df_all = data_df.drop(drop_labels, axis=1).copy(deep=True)\n",
    "    train, test = train_test_split(df_all, train_size=0.8, random_state=100)\n",
    "    train, valid = train_test_split(train, train_size=0.8, random_state=100)\n",
    "    print(train.shape, valid.shape, test.shape)\n",
    "    print(train[label_col].value_counts(True))\n",
    "    print(test[label_col].value_counts(True))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    obj_func = 'binary'\n",
    "    num_rounds = 1000\n",
    "    early_stopping_rounds = 50\n",
    "\n",
    "    print(\"Tune hyperparameters...\")\n",
    "    param_set = {\n",
    "        \"num_leaves\":None, \n",
    "        \"min_data_in_leaf\":None, \n",
    "        \"learning_rate\":None, \n",
    "        \"feature_fraction\":None,\n",
    "        \"bagging_freq\":None, \n",
    "        \"bagging_fraction\":None\n",
    "    }\n",
    "\n",
    "    fixed_params = {\n",
    "        \"objective\":obj_func,\n",
    "        \"metric\":[obj_func, \"auc\"],\n",
    "        \"num_rounds\":num_rounds,\n",
    "        \"early_stopping_rounds\":early_stopping_rounds,\n",
    "        \"first_metric_only\":True,\n",
    "        \"force_row_wise\":True,\n",
    "        \"feature_pre_filter\":False,\n",
    "        \"verbose\":1,\n",
    "    }\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(10, 0.001)\n",
    "\n",
    "    study = tune_model(\n",
    "                        train.drop(columns=[\"date\"]),\n",
    "                        valid.drop(columns=[\"date\"]),\n",
    "                        categoricals, fixed_params, param_set, n_trials=100, verbose_eval=0,\n",
    "                        show_progress=False, early_stop_callback=early_stopping,\n",
    "                    )\n",
    "\n",
    "    print(\"Saving best model parameters...\")\n",
    "    best_params = {k: [v] for (k,v) in study.best_params.items()}\n",
    "\n",
    "    print('best parameters:', best_params)\n",
    "    num_rounds = study.best_trial.user_attrs[\"best_iteration\"]\n",
    "\n",
    "    fixed_params[\"num_rounds\"] = num_rounds\n",
    "    # fixed_params[\"early_stopping_rounds\"] = 0\n",
    "    params = study.best_params.copy()\n",
    "\n",
    "    params.update(fixed_params)\n",
    "    del params[\"early_stopping_rounds\"] # = 0        \n",
    "\n",
    "    params['verbose'] = 1\n",
    "    params['metric'] = ['binary', 'auc']\n",
    "    # params['is_unbalance'] = True\n",
    "    print(params)\n",
    "    \n",
    "    model = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                               num_leaves=params['num_leaves'], \n",
    "                               min_data_in_leaf=params['min_data_in_leaf'],\n",
    "                               learning_rate=params['learning_rate'],\n",
    "                               feature_fraction=params['feature_fraction'],\n",
    "                               bagging_freq=params['bagging_freq'],\n",
    "                               bagging_fraction=params['bagging_fraction'],\n",
    "                               objective='binary',\n",
    "                               metric=params['metric'],\n",
    "                               num_rounds=params['num_rounds'],\n",
    "    #                            is_unbalance=params['is_unbalance']\n",
    "                              )\n",
    "    x_train, y_train = train.drop(columns=[label_col, \"date\"]), train[label_col]\n",
    "    x_val, y_val = valid.drop(columns=[label_col, \"date\"]), valid[label_col]\n",
    "\n",
    "    model.fit(X=x_train, y=y_train, \n",
    "              eval_set=[(x_val, y_val)],\n",
    "              eval_names=['eval']\n",
    "             )     \n",
    "\n",
    "    # Evaluation\n",
    "    x_test, y_test = test.drop(columns=[label_col, \"date\"]), test[label_col]\n",
    "\n",
    "    train_pred = model.predict_proba(x_train)\n",
    "    val_pred = model.predict_proba(x_val)\n",
    "    test_pred = model.predict_proba(x_test)\n",
    "    print(train_pred.shape, val_pred.shape, test_pred.shape)\n",
    "\n",
    "    # AUC\n",
    "    train_auc = roc_auc_score(y_true=y_train, y_score=train_pred[:,1])\n",
    "    val_auc = roc_auc_score(y_true=y_val, y_score=val_pred[:,1])\n",
    "    test_auc = roc_auc_score(y_true=y_test, y_score=test_pred[:,1])\n",
    "    print(f\"train-auc: {train_auc}, val-auc: {val_auc}, test-auc: {test_auc}\")\n",
    "\n",
    "    # Feature Importance\n",
    "    num = 10\n",
    "    feature_imp = pd.DataFrame({'Value':model.feature_importances_,\n",
    "                                'Feature':train.drop(columns=[label_col, \"date\"]).columns})\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.set(font_scale = 1.5)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:num])\n",
    "    plt.title(f'Feature Importance - {t} prediction')\n",
    "    plt.tight_layout()\n",
    "#     plt.savefig('lgbm_importances-01.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Test Precision-Recall Curve\n",
    "    pos_label = 1\n",
    "    average_precision = average_precision_score(y_test, test_pred[:,1])\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, test_pred[:,1])\n",
    "    line_kwargs1 = {\"drawstyle\": \"steps-post\", 'label': 'precision'}\n",
    "    line_kwargs2 = {\"drawstyle\": \"steps-post\", 'label': 'threshold'}\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.set(font_scale = 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, **line_kwargs1)\n",
    "    ax.plot(recall[:-1], thresholds, **line_kwargs2)\n",
    "    info_pos_label = (f\" (Positive label: {pos_label})\"\n",
    "                      if pos_label is not None else \"\")\n",
    "    xlabel = \"Recall\" + info_pos_label\n",
    "    ylabel = \"Precision\" + info_pos_label\n",
    "    title_txt = f\"{t} Prediction - Average Precision = {average_precision:0.2f}\"\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title_txt)\n",
    "    ax.legend(loc=\"lower left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# t = 7502\n",
    "# features = ['power_max8640', 'power_max6048', 'power_max4032', 'power_max2016']\n",
    "\n",
    "t = 7501\n",
    "# features = ['power_max8640', 'power_max6048', 'power_max4032', 'power_mean8640']\n",
    "# limits = [(300,500), (300,500), (300,500), (10,200)]\n",
    "features = ['power_median2016', 'power_median288', 'power_median6048', 'power_median4032']\n",
    "limits = [(0,50), (0,50), (0,50), (0,50)]\n",
    "\n",
    "# t = 1500\n",
    "# features = ['power_max288', 'power_max4032', 'power_max8640', 'power_max6048', 'temp2']\n",
    "drop_labels = ['label_'+str(tc) for tc in target_codes if tc != t]\n",
    "label_col = 'label_' + str(t)\n",
    "df_all = data_df.drop(drop_labels, axis=1).copy(deep=True)\n",
    "\n",
    "for col, lim in zip(features, limits):\n",
    "    df = df_all[[col, label_col]]\n",
    "    print(df[label_col].value_counts())\n",
    "    print(df.groupby(label_col).agg(np.mean))\n",
    "\n",
    "    data1 = df[df[label_col]==1][col]\n",
    "    data2 = df[df[label_col]==0][col]\n",
    "\n",
    "    density1 = gaussian_kde(data1)  # positive\n",
    "    density2 = gaussian_kde(data2)  # negative\n",
    "\n",
    "    xs = np.linspace(lim[0], lim[1], 100)\n",
    "    density1.covariance_factor = lambda : .25\n",
    "    density1._compute_covariance()\n",
    "\n",
    "    density2.covariance_factor = lambda : .25\n",
    "    density2._compute_covariance()\n",
    "    plt.plot(xs, density1(xs), label='positive')\n",
    "    plt.plot(xs, density2(xs), label='negative')\n",
    "    plt.title(f'Density plot: {col} for Alarm-{t}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(model.booster_).shap_values(x_test)\n",
    "shap_values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_importances = np.abs(shap_values[0]).mean(0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_test\n",
    "# make a bar chart that shows the global importance of the top 20 features\n",
    "inds = np.argsort(-global_importances)\n",
    "f = plt.figure(figsize=(5,10))\n",
    "y_pos = np.arange(20)\n",
    "inds2 = np.flip(inds[:20], 0)\n",
    "plt.barh(y_pos, global_importances[inds2], align='center', color=\"#1E88E5\")\n",
    "plt.yticks(y_pos, fontsize=13)\n",
    "plt.gca().set_yticklabels(data.columns[inds2])\n",
    "plt.xlabel('mean abs. SHAP value (impact on model output)', fontsize=13)\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"power_median2016\", shap_values[0], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
